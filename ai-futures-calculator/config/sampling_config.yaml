initial_progress: 0.0
time_range: [2012.0, 2050.0]

num_rollouts: 1000
input_data: input_data.csv
per_sample_timeout: 5

parameters:
  # Superhuman coder time horizon + extrapolation
  ac_time_horizon_minutes:
    dist: lognormal
    ci80: [12456, 2.8e10] # [.1 work year, 225,000 work years]
    min: 960 # demand that superhuman coder have a time horizon of at least 16 hours, which is the length of the longest existing METR tasks
    max: 1e30
  horizon_extrapolation_type:
    dist: fixed
    value: decaying doubling time

  # Gaps mode
  pre_gap_ac_time_horizon:
    dist: lognormal
    ci80: [12456, 1245600] # [.1 work year, 10 work years]
    min: 26 # demand that pre-gap time horizon be at least GPT-5's time horizon
    max: 1e30
  include_gap:
    dist: choice
    values: ["no gap", "gap"]
    p: [0.55, 0.45]
  gap_years:
    dist: lognormal
    ci80: [0.3, 7.5]
    min: 0.0
    max: 1e30

  # Manual horizon fitting (used when horizon_extrapolation_type="decaying doubling time")
  present_day:
    dist: fixed
    value: 2025.6
  present_horizon:
    dist: fixed
    value: 26.0
  present_doubling_time:
    dist: lognormal
    ci80: [0.29, 0.717]
    min: 0.0
    max: 1e30
  doubling_difficulty_growth_factor:
    dist: normal
    ci80:
    - 0.82  
    - 1.02  
    min: 0.6
    max: 1.02

  # coding labor
  rho_coding_labor:
    dist: choice
    values: [-5, -2, -1]
    p: [0.25, 0.5, 0.25]

  # experiment capacity
  inf_compute_asymptote:
    dist: shifted_lognormal
    ci80: [25, 40000]
    shift: 1.0
    min: 1.0
    max: 1e30
  inf_labor_asymptote:
    dist: shifted_lognormal
    ci80: [1, 200]
    shift: 1.0
    min: 1.0
    max: 1e30
  inv_compute_anchor_exp_cap:
    dist: shifted_lognormal
    ci80: [0.8, 5.3]
    shift: 1.0
    min: 1.0
    max: 10
  parallel_penalty:
    dist: beta
    alpha: 3
    beta: 3
    min: 0.0
    max: 1.0

  # coding automation
  swe_multiplier_at_present_day:
    dist: shifted_lognormal
    ci80: [0.18, 2]
    shift: 1.0
    min: 1.0
    max: 1e30
  coding_automation_efficiency_slope:
    dist: lognormal
    ci80: [0.67, 6]
    min: 0.1
    max: 1e30
  max_serial_coding_labor_multiplier:
    dist: lognormal
    ci80: [1e4, 1e12]
    min: 1.0
    max: 1e30
  automation_interp_type:
    dist: fixed
    value: "linear"

  # Research taste dynamics (mixed distributions with rank correlation)
  ai_research_taste_at_coding_automation_anchor_sd:
    dist: normal
    ci80: [-2.5, 3.5]
    clip_to_bounds: false
  ai_research_taste_slope:
    dist: lognormal
    ci80: [0.7, 6.9]
    clip_to_bounds: false
  median_to_top_taste_multiplier:
    dist: shifted_lognormal
    ci80: [0.7, 10.4]
    shift: 1.0
    clip_to_bounds: false
  taste_limit:
    dist: lognormal
    ci80: [2,32]
    clip_to_bounds: false
  taste_limit_smoothing:
    dist: beta
    alpha: 2
    beta: 2
    clip_to_bounds: false

  # Software efficiency growth rate in reference year (2024)
  software_progress_rate_at_reference_year:
    dist: lognormal
    ci80: [0.4, 2.5]
    clip_to_bounds: false

  # Translation to general capability milestones
  ted_ai_m2b:
    dist: lognormal
    ci80: [1, 9]
    min: 0
    max: 1e30

# Correlation matrix specification
# This creates correlations between related parameters
correlation_matrix:
  parameters:
    - "present_doubling_time"
    - "doubling_difficulty_growth_factor"
    - "ai_research_taste_slope"          
    - "ai_research_taste_at_coding_automation_anchor_sd"
    - "gap_years"
    - "pre_gap_ac_time_horizon"
    - "slowdown_year"
    - "ac_time_horizon_minutes"
    - "coding_automation_efficiency_slope"
    - "inv_compute_anchor_exp_cap"
    - "inf_compute_asymptote"
  correlation_matrix:
    - [ 1.00,  0.60, -0.30,  0.00,  0.00,  0.00, -0.30,  0.00, -0.15,  0.00,  0.00] # present_doubling_time
    - [ 0.60,  1.00, -0.30,  0.00,  0.30,  -0.50,  0.00,  -0.50, -0.10,  0.00,  0.00] # doubling_difficulty_growth_factor
    - [-0.30, -0.30,  1.00,  0.30,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00] # ai_research_taste_slope
    - [ 0.00,  0.00,  0.30,  1.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00] # ai_research_taste_at_coding_automation_anchor_sd
    - [ 0.00,  0.30,  0.00,  0.00,  1.00, -0.70, -0.30,  0.00,  0.00,  0.00,  0.00] # gap_years
    - [ 0.00,  -0.50,  0.00,  0.00, -0.70,  1.00,  0.00,  0.00,  0.00,  0.00,  0.00] # pre_gap_ac_time_horizon
    - [-0.30,  0.00,  0.00,  0.00, -0.30,  0.00,  1.00, -0.30,  0.00,  0.00,  0.00] # slowdown_year
    - [ 0.00,  -0.50,  0.00,  0.00,  0.00,  0.00, -0.30,  1.00,  0.00,  0.00,  0.00] # ac_time_horizon_minutes
    - [-0.15, -0.10,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  1.00,  0.00,  0.00] # coding_automation_efficiency_slope
    - [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  1.00,  0.50] # inv_compute_anchor_exp_cap (marginal behavior)
    - [ 0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.00,  0.50,  1.00] # inf_compute_asymptote (C_inf)
  correlation_type: "spearman"
# Input time series uncertainty
# These parameters control the generation of time series data with uncertainty
time_series_parameters:
  constant_training_compute_growth_rate:
    dist: normal
    ci80: [0.55, 0.65]  # Training compute growth rate before slowdown
    min: 0.0
    max: 1e30
  slowdown_year:
    dist: uniform
    min: 2026.0
    max: 2030.0
    clip_to_bounds: false
  post_slowdown_training_compute_growth_rate:
    dist: normal
    ci80: [0.15, 0.35]  # Training compute growth rate after slowdown
    min: 0.0
    max: 1e30
