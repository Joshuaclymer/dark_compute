\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{verbatim}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}

% Define a new sectioning level: subsubsubsection
\titleclass{\subsubsubsection}{straight}[\subsubsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}

\newcommand{\BTODO}[1]{\textcolor{brown}{BH-TODO: #1}}
\newcommand{\BSUG}[1]{\textcolor{orange}{BH-SUGGESTION: #1}}
\newcommand{\ETODO}[1]{\textcolor{blue}{ELI-TODO: #1}}
\newcommand{\ESUG}[1]{\textcolor{purple}{ELI-SUGGESTION: #1}}
\newcommand{\ENOTE}[1]{\textcolor{purple}{ELI-NOTE: #1}}
\newcommand{\ATODO}[1]{\textcolor{red}{ALEX-TODO: #1}}
\newcommand{\ASUG}[1]{\textcolor{magenta}{ALEX-SUGGESTION: #1}}
\newcommand{\ANOTE}[1]{\textcolor{magenta}{ALEX-NOTE: #1}}

\newcommand{\codinganchor}{coding automation anchor}

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}

\titlespacing*{\subsubsubsection}
  {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Make sure LaTeX numbers and shows them in the TOC
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}

\setlength{\belowdisplayskip}{1.5\baselineskip}
\setlength{\belowdisplayshortskip}{1.0\baselineskip}

\begin{document}

\section{Time horizon and the Automated Coder milestone} \label{Time horizon}

Stage 1 of our model primarily predicts the trajectory of AIs' coding skills. We focus on coding automation due to coding being a strength of current AIs relative to other AI R\&D tasks (and we focus on AI R\&D tasks so that we can use them to predict how AI R\&D automation will affect AI progress).

We extrapolate capabilities on the \href{https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/}{METR-HRS coding time horizon benchmark} in order to predict coding automation. We choose METR-HRS due to the relative feasibility of extrapolating it to very advanced capabilities, and the years-long time period for which we have past data.

The coding time horizon, $H(t)$, is the length of the coding tasks the frontier model can successfully complete 80\% of the time, as measured by how long it takes humans to complete them. For example, if an AI has a 2 hour 80\% time horizon, that means it can complete with an 80\% success rate tasks that take human baseliners 2 hours (and for tasks that take baseliners more time, it has a lower success rate). METR has found that time horizon has been growing exponentially over the last 5 years, with a doubling time of roughly 7 months.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{METR_graph.png}
\end{figure}

We want to predict when the \textit{Automated Coder (AC)} milestone is reached. An AC is a collective of AIs that, if dropped into the present day, would be as productive at coding as all present human coders with no AI assistance (using 5\% of the compute of a present-day frontier AI project). A simple method for predicting the arrival date of an Automated Coder would be:

\textbf{Simple Method for forecasting Automated Coder.} \textit{Estimate what time horizon threshold (on an extended version of METR's benchmark) would correspond to an Automated Coder, then use a straightforward exponential-in-time extrapolation of METR's past data points to predict when this time horizon will be achieved.}

\textbf{Expandable: Why an 80\% success rate requirement?} 80\% is the highest success rate time horizon that METR considers high-quality. How is it possible that an 80\% time horizon could correspond to dominating the average coder across all categories of tasks? (a) There is some randomness in whether an AI succeeds at an individual task, so even an AI that dominates average humans across all categories will not have 100\% reliability at human-level or above efficiency on individual tasks. (b) We adjust the required 80\% time horizon upward to account for increased reliability requirements. \textbf{End of expandable}

\textbf{Expandable: Efficiency requirements for time horizons.}  We impose a constraint that the AI must accomplish tasks as efficiently as human researchers for a task completion to count as a success. See \hyperref[Coding automation fraction]{Coding Automation Fraction} for details about what we mean by ``efficient automation". The METR paper does not set an efficiency threshold, but we consider it necessary to set an efficiency threshold, given that AIs already sometimes accomplish tasks less efficiently than humans, and we expect this to become more important over time as AIs are better at translating more resources into better performance.  \textbf{End of expandable}

\textbf{Expandable: Defining our theoretical benchmark METR-HRS-Extended.} Since METR-HRS only contains tasks that take human baseliners $\leq$ 30 hours to complete and we want to forecast up to much higher levels, we define a procedure that could in principle be used to extend the task suite even though it would not be practical to actually do so. It’s important to define which human baseliners are used during the extension: the more competent they are, the larger an impact we should expect at any given time horizon. We assume that METR finds human baseliners who take the same amount of time to complete tasks as a typical AGI company programmer who does similar tasks. We think that this might be loosely consistent with their past baselining, and this also represents a simple assumption that translates relatively well to real-world impacts. \textbf{End of expandable}

However, the Simple Method misses out on some important factors that we want to take into account, namely:
\begin{enumerate}
\item \textbf{The ``time horizon vs calendar time" trend may be disrupted by changes in the growth of inputs to AI progress and the impact of AI automation.} For example, the growth of training compute growth may slow due to limits on investment and the speed of building new fabs. Meanwhile, the human labor growth may slow over time (``there are only so many people you can usefully hire"), but the AIs getting better at coding and research will increase the aggregate labor working on AI R\&D.
\item \textbf{Time horizon may grow superexponentially or subexponentially in calendar time, even if input trends keep holding.} Although METR has observed roughly exponential time horizon growth, with successive time horizon doublings taking roughly the same amount of time, the trend could become superexponential or subexponential (in fact the trend looks slightly superexponential at the moment, but we don't consider this as substantial evidence either way). Our best guess is that time horizon would grow superexponentially even absent AI R\&D automation, mainly because of the ``long-horizon agency argument": as an AI learns to complete increasingly long tasks, it will eventually develop better long-horizon agency skills (long-term planning, decomposing long tasks, noticing and correcting mistakes, etc.) than the human baseliners. On the other hand, one possible reason to expect subexponential growth, at least in the near-term, is data bottlenecks: it may be expensive to collect high-quality data for long-horizon tasks.
\end{enumerate}

The next subsections explain the additions we make to the Simple Method (which already includes a present doubling time\footnote{In our formulas, $\tau$ actually represents the present-day increase in log(effective compute) needed to double time horizon. While website users enter a doubling time parameter (in units of time), this is then automatically converted to the analogous quantity for log(effective compute) that appear in the formulas.} parameter $\tau$ and an Automated Coder time horizon requirement $H_{\text{AC}}$) to address these shortcomings. Here's a brief summary:
\begin{enumerate}
    \item We track time horizon as a function of effective compute (which is a combination of training compute and software progress) rather than calendar time. In particular, to find the AC arrival time, the logic of our model is to first determine the effective compute $E_{\text{AC}}$ corresponding to AC, then later find the AC arrival time by separately modeling the growth of effective compute.
    \item We add a doubling difficulty growth factor $d$ which determines whether (and the extent to which) time horizon grows superexponentially or subexponentially in $\log(\text{effective compute})$. Each time horizon doubling requires an increase in log(effective compute) that's $d$ times larger than the previous doubling. (Note that we've converted the question of time horizon growing (super/sub)exponentially with calendar time to the question of time horizon growing (super/sub)exponentially with log(effective compute).)
    \item We add an optional effective compute ``gap" that is required to reach AC despite the required time horizon having been reached. This represents the gap between succeeding on very long METR-style benchmark coding tasks and automating coding in the real world.
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{time_horizon_growth.png}
\end{figure}

\subsection{Replacing calendar time with effective compute} \label{Replacing calendar time}

%\textbf{Eli's version:}

%Rather than mapping directly from calendar time to AI capabilities, we'd like to track an input variable that takes into account inputs to AI progress, i.e. compute and labor, changing over time. We use \textit{effective compute}: a metric that collapses \textit{software progress}, i.e. more efficiently converting training compute into performance, onto the same axis as training compute.

%See \S\ref{sec:modeling-effective-compute} for more on how effective compute is defined and calculated.

%--- END OF Eli's proposal ---

As mentioned above, we want to move away from directly thinking of time horizon as a function of calendar time, since calendar time trends aren't mechanistic enough for our purposes and they may change in the future (e.g. because training compute growth slows due to investment limits, AIs helping with R\&D introduces new dynamics, etc.). We chose to instead track time horizon (and other AI capabilities) as a function of \textit{effective compute}, which is directly sensitive to changing inputs to AI progress. By definition, effective compute $E(t)$ is the amount of training compute we'd need to train models as performant as the frontier models at time $t$ using the training process of the present-day. It's a convenient metric that collapses software progress (i.e. more efficiently converting training compute into performance) onto the same axis as training compute. We'll revisit the motivation and modeling of effective compute \hyperref[Effective compute]{later}, after we finish explaining our modeling of time horizons and the Automated Coder milestone.

%We switch from modeling time horizon as a function of calendar time, to modeling time horizon as a function of \textit{effective compute}. By definition, effective compute $E(t)$ is the amount of training compute we'd need to train models as performant as the frontier models at time $t$ using the training process of the present-day. It's a useful measure of AI progress which captures the idea that scaling training compute would in principle result in arbitrarily high AI capabilities \ASUG{More motivation for compute-centric view?} Effective compute is the product of training compute $C_\text{train}(t)$ and \textit{software efficiency} $S(t)$:
%\[ E(t) = C_\text{train}(t) \, S(t).\]
%Here $S(t)$  measures how compute-efficient the training process at time $t$ is. By definition, $S(\text{present-day}) = 1$. And $S(\text{June 2027}) = 1000$ would mean: if we used the present-day training process, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027. (We'll come back to how $S(t)$ is modeled later.)

%Effective compute plays an important role in our model. As we explain later, our model attempts to describe how changes in compute, human labor, and AI automation ultimately lead to increases in software efficiency, and in turn in effective compute. So, whereas modeling time horizon as a function of calendar time doesn't account for these changes in compute, human labor, and AI automation, modeling time horizon as a function of effective compute does. And as explained later, we will map increases in effective compute to increases in AIs' capability to automate coding and research.

\subsection{Time horizon progression} \label{Time horizon progression}

Both the METR-HRS time horizon and effective compute increased roughly exponentially over the last 5 years. Therefore one might expect an exponential relationship between the log of effective compute $\log(E(t))$ and time horizon $H(t)$. We write $\tau$ for the number of orders of magnitude (OOMs) of effective compute needed to double time horizon.\footnote{Although we initially talked about time horizon growing exponentially \emph{as a function of time}, the discussion translates to talking about time horizon as a function of (OOMs of) effective compute.}  Intuitively:
\[ \text{effective compute increases by $\tau$ OOMs} \quad \xrightarrow{\text{assumes exponential}} \quad \text{1 doubling of time horizon }\]

We primarily set $\tau$ by fitting historical METR-HRS trends, balancing between the long-term observed trend and a potential recent uptick.

Now we address the second issue about the Simple Method we brought up, namely that the Simple Method assumes time horizon will keep growing exponentially as a function of $\log(E(t))$. We model the possibility that time horizon may grow superexponentially or subexponentially via a parameter $d > 0$, which we call the doubling difficulty growth factor. The idea is that each time horizon doubling should require $d$ times what the last time horizon doubling took (in OOMs of effective compute). So $d=1$ corresponds to the exponential case, $0 < d < 1$ to the superexponential case, and $d > 1$ to the subexponential case. 

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{doubling_difficulty.png}
\end{figure}

Our best guess is that time horizon should grow superexponentially with effective compute, so that $d < 1$. This is primarily because we expect that once AIs surpass humans at long-horizon agency, they will have infinite or extremely large horizons, which imply superexponential growth. \ETODO{Re-write this brief argument and the corresponding expandable. Mention that we don't think all measures will go to inf.}

\textbf{Expandable: When AIs develop better long-horizon agency skills than humans, their success rates may not decrease even as tasks get longer.} 
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{infinite_horizons.png}
\end{figure}
Suppose we get AIs with better long-horizon agency skills than the METR-HRS-Extended human baseliners. If we plot the AIs' success rate on tasks vs. the time it takes the human baseliners to complete those tasks (the ``human-task-time"), what does that look like? There are three possibilities:
\begin{enumerate}
\item \textbf{The success rate vs. human-task-time trend ``flips" to AIs having higher success rates at higher human-task-times, leading to an infinite 80\% time horizon}. This could happen because AIs are now differentially better at long-horizon tasks than humans, as opposed to short-horizon tasks (recall that time horizons are defined relative to humans, as opposed to some inherent property of the task). In this case, we think this should be interpreted as the METR-HRS-Extended time horizon being infinity in finite effective compute and that therefore we should expect superexponential time horizon growth leading up to this.
\item \textbf{The success rate decreases with human-task-time, but it asymptotes above 80\%, leading to an infinite 80\% time horizon.} This could happen because, despite having better long-horion skills than the human baseliners, the AIs may have even better short-horizon skills.
\item \textbf{The success rate drops below 80\% but for extremely long human-task-times.} In this case, the 80\% time horizon vs effective compute trend could be either exponential or superexponential, but there wouldn't be a level of effective compute where the AIs have infinite 80\% time horizon. The fact that the time horizons are extremely large suggests that it would still most likely be superexponential, based on some back-of-the-envelope calcluations.%\ATODO{Include footnote that describes the calculations Eli described in the GDoc.}
\end{enumerate}
\textbf{End of expandable}

We write $\overline{E}$ for log(effective compute) in the formulas in the expandable boxes.

\textbf{Expandable: Achievement of infinite time horizon when time horizon growth is superexponential.} If $0 < d < 1$, i.e. time horizon growth is superexponential, we get an infinite time horizon when
\[ \overline{E} = \overline{E}(\text{present-day}) + \frac{\tau}{1-d}.\]
As discussed previously, an infinite time horizon implies that AIs are more competent at long-horizon agency than human baseliners, \textit{not} that they have reached the limits of intelligence. \textbf{End of expandable}

\textbf{Expandable: Explicit formula for time horizon including superexponential and subexponential.}
Starting from $H(\text{present-day})$, the first doubling of time horizon requires $\overline{E}$ to increase by $\tau$, the second doubling by $d \tau$, the third doubling by $d^2 \tau$, and so forth. Provided $H(t)$ is not infinite yet (which eventually happens for $d < 1$), we can calculate an explicit formula for $H(t)$:
\[ H(t) = 
\begin{cases} 
H(\text{present-day}) \cdot 2^{\frac{\overline{E}(t)-\overline{E}(\text{present-day})}{\tau}} & \text{if $d = 1$}; \\
H(\text{present-day}) \cdot \left(1 - (1-d) \frac{\overline{E}(t)-\overline{E}(\text{present-day})}{\tau}\right)^{\log_d 2} & \text{if $d \neq 1$.}
\end{cases}
\]
\textbf{End of expandable}


\subsection{Setting the effective compute corresponding to the Automated Coder} \label{Automated Coder effective compute}

Now that we've defined how time horizon progresses as a function of effective compute, we need to translate this into the effective compute required to reach an Automated Coder.

We first set the time horizon $H_{\text{AC}}$ required to achieve AC. We set the AC time horizon requirement by estimating the time horizon of tasks that AI company programmers do, adjusting upward because our time horizons only require $>80\%$ success rate, then adjusting upward again for the difference between the METR-HRS-Extended benchmark and real-world coding tasks.

Finally, we add an optional ``effective compute gap" to our model to represent the possibility that the Automated Coder milestone may come later than when the AC time horizon requirement is achieved (e.g. maybe AIs will saturate METR-HRS-extended without fully replacing a human on real-world coding tasks, in a way that can't be corrected for by adjusting the required horizon). For simulations of our model that use a gap, we sample the ``pre-gap time horizon" from a different distribution than the distribution we use for simulations where the AC effective compute requirement does not use a gap. We do this because the ``gap perspective" seems connected to the view that an extended METR-style benchmark would saturate at lower time horizons, compared with the perspective where a high enough time horizon gives an Automated Coder.

% \ASUG{Still a bit confusing what ``saturates at a low time horizon" means for the METR benchmark.}

\textbf{Expandable: Explicit formula for the Automated Coder effective compute requirement, including an (optional) effective compute gap.}
\[     \overline{E}_{\text{AC}}=
    \begin{cases}
        \overline{E}(\text{present-day}) + \frac{\tau}{d-1}\left(\left(\frac{H_{\text{AC}}}{H(\text{present-day})}\right)^{\log_2 d}-1\right) + \overline{E}_{\text{gap}} & \text{ if } d \neq 1; \\
        \overline{E}(\text{present-day}) + \tau \,\log_2\!\left(\frac{H_{\text{AC}}}{H(\text{present-day})}\right) + \overline{E}_{\text{gap}} & \text{ if } d=1.
    \end{cases}\]
\textbf{End of expandable}

\begin{comment}
\section{Modeling effective compute, version 1}
\label{sec:modeling-effective-compute}

%\ASUG{Do we want to call ``automated labor" an input?}

In our model, we take into account inputs including human labor, automated labor, training compute, experiment compute, and automation compute. We therefore need some way to aggregate all these inputs together and map them onto concrete capabilties. Given that recent AI progress has been driven in substantial part by scaling up the compute used to train models, we decide to aggregate these inputs into \textit{effective compute}: a metric that collapses \textit{software progress}, i.e. more efficiently converting training compute into performance, onto the same axis as training compute. We then translate effective compute into concrete capabilities, as described in \hyperref[Automated Coder effective compute]{Automated Coder Effective Compute}, \hyperref[Coding automation]{Coding Automation}, and \hyperref[Experiment selection automation]{Experiment Selection Automation}.

Effective compute $E(t)$ is the amount of training compute we'd need to train models as performant as the frontier models at time $t$ using the training process of the present-day. We express effective compute as the product of training compute $C_\text{train}(t)$ and \textit{software efficiency} $S(t)$:
\[ E(t) = C_\text{train}(t) \, S(t).\]
Software efficiency $S(t)$ measures how compute-efficient the training process at time $t$ is. By definition, $S(\text{present-day}) = 1$. And $S(\text{June 2027}) = 1000$ would mean: if we used the present-day training process, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027. (We'll come back to how $S(t)$ is modeled later.)

While effective compute is the best solution we know of for aggregating inputs to AI progress, it bakes in important assumptions, in particular that something like current AI training processes with relatively minor adaptations could scale to very high capabilities. We believe our model is still valuable even if this assumption fails, as one could make definitions of effective compute that allow for more ``adaptation labor" and this might lead to similar behavior. But it is certainly harder to interpret the effective compute values without this assumption and our model's results may be off base because of this.\footnote{In future model iterations, we may switch to using something like the Epoch Capabilities Index as our core capabilities metric.} \ASUG{``Adaptation labor" needs to be clarified.}
%\ESUG{Maybe move above paragraph to expandable.} 

\subsection{Compute forecasts} \label{sec:compute-forecasts}

In our model we track 3 types of compute:
\begin{enumerate}
    \item Training compute $C_\text{train}(t)$, a direct input to effective compute.
    \item Experiment compute $C_\text{xpm}(t)$, an input to software efficiency.
    \item Automation compute $C_\text{aut}(t)$, an input into \hyperref[Coding automation]{coding automation}.
\end{enumerate}

These are modeled as exogenous time series, i.e. they don't change based on what else is going on in the model. This means we aren't modeling hardware R\&D automation, hardware production automation, or economic growth, which is a serious limitation of our model, particularly in slow takeoff scenarios.

We forecast these by doing \ETODO{finish this}.

\subsection{Software efficiency} \label{sec:software-efficiency}

%\ASUG{I don't like this subsection on software efficiency and prefer my version.}

Recall that software efficiency, $S(t)$, measures how efficiently the training process at time $t$ can convert training compute into performance. The inputs to $S(t)$ in the past have been human labor and experiment compute. We need to model how the inputs of human labor and experiment compute are transformed into the output $S(t)$.

When we look at past empirical data, we see that human labor and experiment compute have been growing exponentially over time. Meanwhile, $S$ has also been growing exponentially.
\ESUG{We should have a figure showing our guess as to each of human labor, exp compute, and S have grown in the past}

\[\text{AI R\&D labor and experiment compute grows exponentially}\]
\[\downarrow\]
\[\text{$S$ grows exponentially}\]

Before modeling the relationship between the inputs to software R\&D and the output, S, we define a way of aggregating the inputs.

\textit{Research effort, $RE$,} aggregates human labor and experiment compute into a single metric. In our model, it can be thought of as the "counterfactual speed" of a project, in that if the project's $RE$ was instantaneously increased by $2$x, they would go $2$x faster. \ENOTE{This was a quick attempt at my current favorite way to define RE}

We should expect $RE$ to be growing exponentially if the inputs to $RE$ are growing exponentially. \ENOTE{does this need justification?}

What does this suggest for how we should model the relationship between $RE$ and $S$? Let's define \textit{research stock RS(t)} as the \textit{total} amount of research effort done by time $t$. In symbols,
\[ RS(t) = \int_{-\infty}^t RE(\tau) \, d\tau\text{; $RS$ is to $RE$ what distance traveled is to speed.}\]

If $RE$ is growing exponentially, $RS$ must also be growing exponentially. Since $S(t)$ and $RS(t)$ both grow exponentially over time, the relationship between them is best modeled by a power law:
\[ S(t) = C \cdot RS(t)^{1/\beta}, \quad \text{for some fixed parameters $C$ and $\beta > 0$}.\]

Intuitively:
\[ \text{$\beta$ doublings of research stock $RS$} \quad \xrightarrow{\text{produces}} \quad \text{1 doubling of software efficiency $S$}\]

In order to keep up with growth in training compute $C_{train}$, $S$ needs to grow exponentially; if $S$ grows subexponentially, then software efficiency improvements will fade into irrelevance relative to training compute.

But in order to sustain exponential growth in $S$, $RE$ must also grow exponentially. Experiment compute $C_{xpm}$ should grow at a similar rate to training compute, so the question of whether $S$ can continue to "keep up" with $C_{train}$ depends on whether human and automated labor can continue at least exponential growth.

\[ \text{exponential increases in AI R\&D labor}\]
\[\downarrow \text{are required for}\]%\text{are required for}
\[\text{exponential growth in $S$ to keep up with $C_{train}$}\]

This can be explained by appealing to ``diminishing returns" to $RE$: as the ``low-hanging fruit" of software improvements get plucked, sustaining exponential growth of $S$ requires increasing amounts of effort. This phenomenon has sometimes been referred to as ``ideas getting harder to find`` [cite bloom].

\textbf{Expandable: Mathematics and visualization of diminishing returns}

AI capabilities come from a combination of software efficiency $S$ and training compute $C_{\text{train}}$. Since training compute has been growing exponentially, for $S$ to ``keep up" and not fade into irrelevance, it also needs to be growing exponentially. So it's natural to talk about returns on research effort in terms of $\overline{S} = \log S$ (i.e. orders of magnitude of $S$). Since
\[ \overline{S} = \log C + \frac{1}{\beta} \cdot \log RS \approx \frac{1}{\beta} \cdot \log RS,\]
successive equal-sized increases in $RS$ lead to smaller and smaller increases in $\overline{S}$ over time. This is the phenomenon of ``diminishing returns": the more progress has already been made, the harder it becomes to make impactful new discoveries since the low-hanging fruits have been picked.

\begin{figure}[H]
    \centering
     \includegraphics[width=0.8\linewidth]{diminishing_returns.png}
\end{figure}
\textbf{End of expandable}

\textbf{Expandable: Connection to the Jones semi-endogenous growth model}

Jones 1995 uses the following growth law to model idea production:
\[
A'(t) = a \, A(t)^{1-\beta} \, R(t)^\lambda.
\]
Here $A(t)$ is the stock of ideas and $R(t)$ is the number of researchers engaged in idea production. The parameter $\lambda > 0$ controls the ``stepping on toes'' effect from adding additional researchers, and $\beta>0$ represents the strength of the ``fishing out'' effect, according to which ideas become harder to find as the stock grows.

In past AGI takeoff models (e.g., \href{https://takeoffspeeds.com/description.html}{Davidson's FTM model} and \href{https://epoch.ai/gate}{Epoch's GATE model}), software efficiency $S(t)$ replaced the stock of ideas $A(t)$. All proposed models have had the form
\[S'(t) = a \, S(t)^{1-\beta} \, RE(t),\]
for various definitions of research effort $RE(t)$.

We show this growth law reduces to our formula $S(t) = C \cdot RS(t)^{1/\beta}$ from above. We have:
\begin{align*}
RS(t) &= \int_{-\infty}^t RE(\tau) \, d\tau = \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau. \\
\intertext{The integral on the right can be computed via the substitution $U = S(\tau)^\beta$, $dU = \frac{\beta S'(\tau)}{ S(\tau)^{1-\beta}}d\tau$:}
RS(t) &= \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau \\
 &= \frac{1}{a\beta} \left(S(t)^\beta - S(-\infty)^\beta\right) \\
 &= \frac{1}{a \beta} S(t)^\beta . \\
\intertext{Solving for $S(t)$,}
S(t) &= \left( a \beta \right)^{1/\beta} RS(t)^{1/\beta} = C \cdot RS(t)^{1/\beta}, \quad \quad \quad \text{for $C = \left(a\beta\right)^{1/\beta}$}. 
\end{align*}
\textbf{End of expandable}

\subsection{Research effort and AI software R\&D uplift} \label{sec:research-effort}

Research effort $RE(t)$ is the number of quality-adjusted experiments the project can perform per unit time. Research effort combines:
\begin{enumerate}
\item the number of experiments the project can implement and run per unit time (where compute-intensive and coding-labor-intensive experiments are weighted more heavily), which we call \textit{experiment throughput} $X(t)$; and
\item the average value of an experiment, which we call (aggregate) \textit{experiment selection skill} $T(t)$ ($T$ for ``taste").
\end{enumerate}
Research effort is then expressed as:
\[ RE(t) = T(t) \, X(t)\]
For example, if researcher A has $2 \times$ the experiment selection skill of researcher B, then an experiment proposed by A counts for twice as much in research effort as an experiment proposed by B (assuming the two experiments are compute-equivalent and coding-labor-equivalent).

For how experiment selection skill is calculated, see \hyperref[Aggregate Experiment Selection Skill]{Aggregate experiment selection skill}. We discuss experiment throughput in the next subsection.

%We model research effort in an experiment-centric fashion.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Research_effort.png}
\end{figure}

Related to research effort, we track \textit{AI software R\&D uplift} for each time $t$ (called ``AI software R\&D progress multiplier” in our previous work). This concept is what it sounds like: it measures ``how much faster R\&D goes with the AIs compared with without the AIs". Write $RE(\text{AI}_t \rightarrow \text{present-day})$ for what research effort would be in the present-day if we magically transported the AIs from time $t$ back to the present-day. Then AI software R\&D uplift is:\footnote{In the definition of AI software R\&D uplift, we only transport \textit{the AIs} back to the present-day, not the extra experiment/automation compute and the extra human labor that will be available at time $t$.}
\[\frac{RE(\text{AI}_t \rightarrow \text{present-day})}{RE(\text{present-day})}.\]

\subsection{Experiment throughput} \label{sec:experiment-throughput}

\textit{Experiment throughput} $X(t)$ represents the AI project's capacity for implementing and running experiments. It can roughly be understood as the number of experiments the project manages to implement and run per unit time, where experiments are weighted by how much compute they use and how labor-intensive they are to code.

Experiment throughput is modeled as a function of experiment compute $C_\text{xpm}(t)$ (i.e. the H100-equivalents used to run research experiments) and coding labor $L_C(t)$ (i.e. the number of full-time-human-equivalent coders). Although experiment throughput goes up with both experiment compute $C_{\text{xpm}}(t)$ and coding labor $L_C(t)$, there are diminishing returns to only increasing one of the inputs and not the other. One way to model this relationship would be to use a Cobb-Douglas function: $X(t) = C_\text{xpm}(t)^\alpha L_C(t)^\beta$ where $0 < \alpha < 1$ and $0 < \beta < 1$. This has the property that if one of $C_\text{xpm}(t)$ or $L_C(t)$ becomes much larger than the other, then increases in $X(t)$ come mainly from the smaller input, because the larger input exhibits diminishing returns.

One problem with the Cobb-Douglas function is that experiment throughput can go to infinity when one of $C_{\text{xpm}}(t)$ or $L_C(t)$ goes to infinity but the other remains bounded, which isn't realistic. If you have infinite coding labor but only a fixed amount of experiment compute, you can only implement so many experiments per unit time; and if you have experiment compute but only a fixed number of coders, you don't have enough workers to code up experiments that (usefully) use the compute.\footnote{Technically, with infinite compute you could code something up to search the space of all possible AI designs, but that takes time and thus does not provide an infinite speedup.} In economic terms, we would say that experiment compute and coding labor are complementary inputs that cannot fully substitute for each other. This is often modeled with a CES (``constant elasticity of substitution") production function with substitution parameter $\rho_X < 0$ (the rest of the model explanation won't depend on the mathematical details we discuss here): 
\[ X(t) = \left( \alpha \, \tilde{C}_{\text{xpm}}(t)^{\rho_X} + (1-\alpha) \, \tilde{L}_C(t)^{\rho_X} \right)^{1/\rho_X}, \quad 0 < \alpha < 1,\; \rho_X < 0,\]
where
\[\tilde C_{\text{xpm}}(t) = C_{\text{xpm}}(t)^\zeta, \quad \tilde L_{\text{C}}(t) = L_{\text{C}}(t)^\lambda, \quad 0 < \zeta < 1,\; 0 < \lambda < 1.\]

\textit{What is $\alpha$?} The closer $0< \alpha < 1$ is to $1$, the more experiment compute is considered important relative to coding labor. 

\textit{What is $\rho_X$?} The smaller $\rho_X < 0$ is, the less it's possible to only increase one of $C_{\text{xpm}}(t)$ and $L_C(t)$ in order to increase $X(t)$. The CES expression has the property that: if we fix $C_\text{xpm}(t)$ and take $L_C(t)$ to infinity, then $X(t)$ will asymptote to $\alpha^{1/\rho} \tilde{C}_\text{xpm}(t)$; and if we fix $L_C(t)$ and take $C_\text{xpm}(t)$ to infinity, then $X(t)$ will asymptote to $(1-\alpha)^{1/\rho} \tilde{L}_C(t)$. In simple terms, when $\rho_X < 0$ is small, the maximum possible boost you can get from increasing one of experiment compute or coding labor to infinity is lower. %\ASUG{Example?}

\textit{What is $\lambda$?} We use the smaller quantity $L_C(t)^\lambda$ instead of $L_C(t)$ since $L_C(t)$ is a measure of \textit{parallel} coding labor but we want a measure of \textit{serial} coding labor (a single programmer can make more progress in two days than a team of two programmers can make in a day because of the ``stepping on toes" effect). The reason we use serial (rather than parallel) coding labor in the CES function is that if the project had unlimited experiment compute, the way we double experiment throughput is by letting the coding workforce work for twice as long (or speeding up the coding workforce by $2 \times$), rather than doubling the size of the coding workforce.

\textit{What is $\zeta$?} The reason we discount experiment compute by the exponent \(0<\zeta<1\) is that, if one had unlimited coding labor, doubling experiment compute doesn't double experiment throughput because model size increases, meaning the ability to run near-frontier experiments goes down.

%\ASUG{I wonder if discussion about how to set parameters should be put elsewhere.}
%\ESUG{Yeah I'm not sure. We will definitely want our parameter estimation rationales somewhere on the site but not sure where they belong, and if they are somewhere else to what extent they should be summarized here. I think I do like things being summarized here a bit.}

We set the parameter values as follows: \begin{enumerate}
    \item $\lambda$: We estimated this based on our intuitions about the effect of increased parallel labor on research effort, as well as interviews with frontier AI researchers.
    \item $\zeta, \alpha, \rho_X$: We estimated three quantities, from a fixed starting point of 2024, that together pin down the values of $\zeta$, $\alpha$, and $\rho_X$: (a) the decrease in $X(t)$ from a $10 \times$ decrease in $C_{\text{xpm}}$, (b) the $X(t)$ asymptote as we take $C_{\text{xpm}}$ to infinity, and (c) the $X(t)$ asymptote as we take $L_C(t)$ to infinity. To estimate these quantities, we combined frontier AI researcher interviews, surveys of other AI experts, and our own reasoning about how $X(t)$ would change given infinite coding labor or compute.
\end{enumerate}
\end{comment}

% Version 2
\section{Modeling effective compute} \label{Effective compute}

%\ASUG{Do we want to call ``automated labor" an input?}

% "collapses software progress" or "collapses software efficiency"?
In our model, we take into account inputs including human labor, automated labor, training compute, experiment compute, and automation compute. We need some way to aggregate all these inputs together and map them onto concrete capabilties. Given that recent AI progress has been driven in substantial part by scaling up the compute used to train models, we decide to aggregate these inputs into \textit{effective compute}: a metric that collapses \textit{software progress}, i.e. more efficiently converting training compute into performance, onto the same axis as training compute. We then model concrete capabilities as functions of effective compute, as described in \hyperref[Automated Coder effective compute]{Automated Coder Effective Compute}, \hyperref[Coding automation]{Coding Automation}, and \hyperref[Experiment selection automation]{Experiment Selection Automation}.

Effective compute $E(t)$ is the amount of training compute we'd need to train models as performant as the frontier models at time $t$ using the training process of the present-day. We express effective compute as the product of training compute $C_\text{train}(t)$ and \textit{software efficiency} $S(t)$:
\[ E(t) = C_\text{train}(t) \, S(t).\]
Software efficiency $S(t)$ measures how compute-efficient the training process at time $t$ is. By definition, $S(\text{present-day}) = 1$. And $S(\text{June 2027}) = 1000$ would mean: if we used the present-day training process, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027.

\textbf{Expandable: Limitations of using effective compute.}
While effective compute is the best solution we know of for aggregating inputs to AI progress, it bakes in important assumptions, in particular that something like current AI training processes with relatively minor adaptations could scale to very high capabilities. We believe our model is still valuable even if this assumption fails, as one could make definitions of effective compute that allow for more ``adaptation labor" and this might lead to similar behavior. But it is certainly harder to interpret the effective compute values without this assumption and our model's results may be off base because of this.\footnote{In future model iterations, we may switch to using something like the Epoch Capabilities Index as our core capabilities metric.} \ASUG{``Adaptation labor" needs to be clarified.}
\textbf{End of expandable}

Our model of the progression of software efficiency $S(t)$ depends on the variables of \textit{experiment throughput} and \textit{(software) research effort}. For clarity of exposition, we discuss these first and how they're modeled. In \hyperref[a later section]{Software efficiency}, we'll explain how gains in research effort are converted into gains in software efficiency. Before all that, we say a bit more about the compute data our model uses.

\subsection{Compute forecasts} \label{Compute}

In our model we track 3 types of compute:
\begin{enumerate}
    \item Training compute $C_\text{train}(t)$, a direct input to effective compute.
    \item Experiment compute $C_\text{xpm}(t)$, an input to software efficiency.
    \item Automation compute $C_\text{aut}(t)$, an input into \hyperref[Coding automation]{coding automation}.
\end{enumerate}

These are modeled as exogenous time series, i.e. they don't change based on what else is going on in the model. This means we aren't modeling hardware R\&D automation, hardware production automation, or economic growth, which is a serious limitation of our model, particularly in slow takeoff scenarios.

We forecast these by doing \ETODO{finish this}.

\subsection{Experiment throughput} \label{Experiment throughput}

\textit{Experiment throughput} $X(t)$ represents the AI project's capacity for implementing and running experiments. It can roughly be understood as the number of experiments the project manages to implement and run per unit time, where experiments are weighted by how much compute they use and how labor-intensive they are to code.

Experiment throughput is modeled as a function of experiment compute $C_\text{xpm}(t)$ (i.e. the H100-equivalents used to run research experiments) and coding labor $L_C(t)$ (i.e. the number of full-time-human-equivalent coders). Although experiment throughput goes up with both experiment compute $C_{\text{xpm}}(t)$ and coding labor $L_C(t)$, there are diminishing returns to only increasing one of the inputs and not the other. One way to model this relationship would be to use a Cobb-Douglas function: $X(t) = C_\text{xpm}(t)^\alpha L_C(t)^\beta$ where $0 < \alpha < 1$ and $0 < \beta < 1$. This has the property that if one of $C_\text{xpm}(t)$ or $L_C(t)$ becomes much larger than the other, then increases in $X(t)$ come mainly from the smaller input, because the larger input exhibits diminishing returns.

One problem with the Cobb-Douglas function is that experiment throughput can go to infinity when one of $C_{\text{xpm}}(t)$ or $L_C(t)$ goes to infinity but the other remains bounded, which isn't realistic. If you have infinite coding labor but only a fixed amount of experiment compute, you can only implement so many experiments per unit time; and if you have experiment compute but only a fixed number of coders, you don't have enough workers to code up experiments that (usefully) use the compute.\footnote{Technically, with infinite compute you could code something up to search the space of all possible AI designs, but that takes time and thus does not provide an infinite speedup.} In economic terms, we would say that experiment compute and coding labor are complementary inputs that cannot fully substitute for each other. This is often modeled with a CES (``constant elasticity of substitution") production function with substitution parameter $\rho_X < 0$ (the rest of the model explanation won't depend on the mathematical details we discuss here): 
\[ X(t) = \left( \alpha \, \tilde{C}_{\text{xpm}}(t)^{\rho_X} + (1-\alpha) \, \tilde{L}_C(t)^{\rho_X} \right)^{1/\rho_X}, \quad 0 < \alpha < 1,\; \rho_X < 0,\]
where
\[\tilde C_{\text{xpm}}(t) = C_{\text{xpm}}(t)^\zeta, \quad \tilde L_{\text{C}}(t) = L_{\text{C}}(t)^\lambda, \quad 0 < \zeta < 1,\; 0 < \lambda < 1.\]

\textit{What is $\alpha$?} The closer $0< \alpha < 1$ is to $1$, the more experiment compute is considered important relative to coding labor. 

\textit{What is $\rho_X$?} The smaller $\rho_X < 0$ is, the less it's possible to only increase one of $C_{\text{xpm}}(t)$ and $L_C(t)$ in order to increase $X(t)$. The CES expression has the property that: if we fix $C_\text{xpm}(t)$ and take $L_C(t)$ to infinity, then $X(t)$ will asymptote to $\alpha^{1/\rho} \tilde{C}_\text{xpm}(t)$; and if we fix $L_C(t)$ and take $C_\text{xpm}(t)$ to infinity, then $X(t)$ will asymptote to $(1-\alpha)^{1/\rho} \tilde{L}_C(t)$. In simple terms, when $\rho_X < 0$ is small, the maximum possible boost you can get from increasing one of experiment compute or coding labor to infinity is lower. %\ASUG{Example?}

\textit{What is $\lambda$?} We use the smaller quantity $L_C(t)^\lambda$ instead of $L_C(t)$ since $L_C(t)$ is a measure of \textit{parallel} coding labor but we want a measure of \textit{serial} coding labor (a single programmer can make more progress in two days than a team of two programmers can make in a day because of the ``stepping on toes" effect). The reason we use serial (rather than parallel) coding labor in the CES function is that if the project had unlimited experiment compute, the way we double experiment throughput is by letting the coding workforce work for twice as long (or speeding up the coding workforce by $2 \times$), rather than doubling the size of the coding workforce.

\textit{What is $\zeta$?} The reason we discount experiment compute by the exponent \(0<\zeta<1\) is that, if one had unlimited coding labor, doubling experiment compute doesn't double experiment throughput because model size increases, meaning the ability to run near-frontier experiments goes down.

%\ASUG{I wonder if discussion about how to set parameters should be put elsewhere.}
%\ESUG{Yeah I'm not sure. We will definitely want our parameter estimation rationales somewhere on the site but not sure where they belong, and if they are somewhere else to what extent they should be summarized here. I think I do like things being summarized here a bit.}

We set the parameter values as follows: \begin{enumerate}
    \item $\lambda$: We estimated this based on our intuitions about the effect of increased parallel labor on research effort, as well as interviews with frontier AI researchers.
    \item $\zeta, \alpha, \rho_X$: We estimated three quantities, from a fixed starting point of 2024, that together pin down the values of $\zeta$, $\alpha$, and $\rho_X$: (a) the decrease in $X(t)$ from a $10 \times$ decrease in $C_{\text{xpm}}$, (b) the $X(t)$ asymptote as we take $C_{\text{xpm}}$ to infinity, and (c) the $X(t)$ asymptote as we take $L_C(t)$ to infinity. To estimate these quantities, we combined frontier AI researcher interviews, surveys of other AI experts, and our own reasoning about how $X(t)$ would change given infinite coding labor or compute.
\end{enumerate}

\subsection{Software research effort and AI R\&D uplift} \label{Research effort}

Research effort $RE(t)$ is the number of \textit{quality-adjusted} experiments the project can perform per unit time. That is, it's a version of experiment throughput $X(t)$ but where experiments proposed by more skillful researchers are weighted more (experiment throughput only weighted experiments by the amount of experiment compute they required and how much coding labor they required to be implemented).

We model research effort $RE(t)$ as the product of experiment throughput $X(t)$ and \textit{aggregate experiment selection skill} $T(t)$ (``$T$" for ``taste"):
\[ RE(t) = T(t) \, X(t.)\]
Intuitively, $T(t)$ measures the average value per experiment. For example, if researcher A has $2 \times$ the experiment selection skill of researcher B, this means an experiment proposed by A is as valuable as two experiments proposed by B (assuming all three of these experiments are compute-equivalent and coding-labor-equivalent). For how experiment selection skill is calculated, see \hyperref[Aggregate Experiment Selection Skill]{Aggregate experiment selection skill}. 

%We model research effort in an experiment-centric fashion.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Research_effort.png}
\end{figure}

Related to research effort, we track \textit{AI software R\&D uplift} for each time $t$ (called ``AI software R\&D progress multiplier” in our previous work). This concept is what it sounds like: it measures ``how much faster R\&D goes with the AIs compared with without the AIs". Write $RE(\text{AI}_t \rightarrow \text{present-day})$ for what research effort would be in the present-day if we magically transported the AIs from time $t$ back to the present-day. Then AI software R\&D uplift is:\footnote{In the definition of AI software R\&D uplift, we only transport \textit{the AIs} back to the present-day, not the extra experiment/automation compute and the extra human labor that will be available at time $t$.}
\[\frac{RE(\text{AI}_t \rightarrow \text{present-day})}{RE(\text{present-day})}.\]


\subsection{Software efficiency} \label{Software efficiency}

Recall that software efficiency, $S(t)$, measures how efficiently the training process at time $t$ can convert training compute into performance. We have $S(\text{present-day}) = 1$, and $S(\text{June 2027}) = 1000$ would mean: if we used the training process of the present-day, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027.

We want to understand how $S(t)$ changes as research effort $RE(t)$ changes. Technically though, $S$ is not a function of $RE$ (which is a measure of how ``fast" the AI project is going), but of how much total work has actually been done so far. So define the research stock, $RS(t)$, to be the \textit{total} amount of research effort done by time $t$; in symbols,
\[ RS(t) = \int_{-\infty}^t RE(\tau) \, d\tau.\]
So research stock is to research effort what distance traveled is to speed.

Both research stock and software efficiency describe ``how far" the AI project has come, but research stock (like research effort) is about effort/investment while software efficiency is about fruits/returns. We want to know their relationship:
\[ \text{$RS \approx$ total AI software R\&D work done} \quad \xrightarrow{\text{relationship?}} \quad \text{$S \approx$ total returns}\]

When we look at past empirical data, we see that $S(t)$ has been growing exponentially over time. Meanwhile research effort and research stock should also be growing exponentially, since both experiment compute and AI software R\&D labor have been growing exponentially over the last 10 years. So, since $S(t)$ and $RS(t)$ both grow exponentially over time (with different bases for the exponentials), the relationship between them is best modeled by a power law: 
\[ S(t) = C \cdot RS(t)^{1/\beta}, \quad \text{for some fixed parameters $C$ and $\beta > 0$}.\]
Intuitively:
\[ \text{$\beta$ doublings of research stock $RS$} \quad \xrightarrow{\text{produces}} \quad \text{1 doubling of software efficiency $S$}\]

\textit{Diminishing returns}: AI capabilities come from a combination of software efficiency $S$ and training compute $C_{\text{train}}$. Since training compute has been growing exponentially, for $S$ to ``keep up" and not fade into irrelevance, it also needs to be growing exponentially. So it's natural to talk about returns on research effort in terms of $\overline{S} = \log S$ (i.e. orders of magnitude of $S$). Since
\[ \overline{S} = \log C + \frac{1}{\beta} \cdot \log RS \approx \frac{1}{\beta} \cdot \log RS,\]
successive equal-sized increases in $RS$ lead to smaller and smaller increases in $\overline{S}$ over time. This is the phenomenon of ``diminishing returns": the more progress has already been made, the harder it becomes to make impactful new discoveries since the low-hanging fruits have been picked.

\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{diminishing_returns.png}
\end{figure}

\textbf{Expandable: Connection to the Jones semi-endogenous growth model}

Jones 1995 uses the following growth law to model idea production:
\[
A'(t) = a \, A(t)^{1-\beta} \, R(t)^\lambda.
\]
Here $A(t)$ is the stock of ideas and $R(t)$ is the number of researchers engaged in idea production. The parameter $\lambda > 0$ controls the ``stepping on toes'' effect from adding additional researchers, and $\beta>0$ represents the strength of the ``fishing out'' effect, according to which ideas become harder to find as the stock grows.

In past AGI takeoff models (e.g., \href{https://takeoffspeeds.com/description.html}{Davidson's FTM model} and \href{https://epoch.ai/gate}{Epoch's GATE model}), software efficiency $S(t)$ replaced the stock of ideas $A(t)$. All proposed models have had the form
\[S'(t) = a \, S(t)^{1-\beta} \, RE(t),\]
for various definitions of research effort $RE(t)$.

We show this growth law reduces to our formula $S(t) = C \cdot RS(t)^{1/\beta}$ from above. We have:
\begin{align*}
RS(t) &= \int_{-\infty}^t RE(\tau) \, d\tau = \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau. \\
\intertext{The integral on the right can be computed via the substitution $U = S(\tau)^\beta$, $dU = \frac{\beta S'(\tau)}{ S(\tau)^{1-\beta}}d\tau$:}
RS(t) &= \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau \\
 &= \frac{1}{a\beta} \left(S(t)^\beta - S(-\infty)^\beta\right) \\
 &= \frac{1}{a \beta} S(t)^\beta . \\
\intertext{Solving for $S(t)$,}
S(t) &= \left( a \beta \right)^{1/\beta} RS(t)^{1/\beta} = C \cdot RS(t)^{1/\beta}, \quad \quad \quad \text{for $C = \left(a\beta\right)^{1/\beta}$}. 
\end{align*}
\textbf{End of expandable}

\begin{comment}

\section{Modeling changes in software efficiency and effective compute (old), AK version}

Next we explain our model of software R\&D and how this determines the progression of software efficiency $S(t)$. Together with an exogenous time series for training compute $C_{\text{train}}(t)$, this determines the progression of effective compute $E(t) = C_{\text{train}}(t) \, S(t)$.

As a first pass, we explain how our model describes software R\&D progress \textit{without the contribution of the AIs}. Later we explain how to account for AI automation, but it will be simpler to first explain things without the AIs. \textbf{Warning: The rest of this section is a simplification of what happens in the full model, and ignores the contribution of R\&D automation.}

\subsection{Experiment throughput} \label{Experiment throughput}

We introduce a variable called \textit{experiment throughput} $X(t)$ to represent the AI project's capacity for implementing and running experiments. It can roughly be understood as the number of experiments the project manages to implement and run per unit time, where experiments are weighted by how much compute they use and how labor-intensive they are to code.

Experiment throughput is modeled as a function of experiment compute $C_\text{xpm}(t)$ (i.e. the compute used to run research experiments) and coding labor $L_C(t)$ (i.e. the number of full-time-equivalent human coders). Although experiment throughput goes up with both experiment compute $C_{\text{xpm}}(t)$ and coding labor $L_C(t)$, there are diminishing returns to only increasing one of the inputs and not the other. One way to model this relationship would be to use a so-called Cobb-Douglas function: $X(t) = C_\text{xpm}(t)^\alpha L_C(t)^\beta$ where $0 < \alpha < 1$ and $0 < \beta < 1$. This would have the property that if one of $C_\text{xpm}(t)$ or $L_C(t)$ becomes much larger than the other, then the growth of experiment throughput starts to bottleneck on the other factor.

We chose to use a slightly more complicated function to relate $X(t)$ and $C_\text{xpm}(t)$ and $L_C(t)$. One problem with the Cobb-Douglas function is that experiment throughput can go to infinity when one $C_{\text{xpm}}(t)$ or $L_C(t)$ goes to infinity but the other remains bounded, which isn't realistic. Indeed, if you have infinite coding labor but only a fixed amount of experiment compute, you can only implement so many experiments per unit time; and if you have experiment compute but only a fixed number of coders, you don't have enough workers to code up experiments that (usefully) use the compute.\footnote{Technically, with infinite compute you could code something up to search the space of all possible AI designs, but that takes time and thus does not provide an infinite speedup.} This situation comes up in economics, where we would say that experiment compute and coding labor are complementary inputs that are hard to substitute for each other. It's often modeled with a so-called CES (``constant elasticity of substitution") production function with substitution parameter $\rho_X < 0$: 
\[ X(t) = \left( \alpha \, \tilde{C}_{\text{xpm}}(t)^{\rho_X} + (1-\alpha) \, \tilde{L}_C(t)^{\rho_X} \right)^{1/\rho_X}, \quad 0 < \alpha < 1,\; \rho_X < 0,\]
where
\[\tilde C_{\text{xpm}}(t) = C_{\text{xpm}}(t)^\zeta, \quad \tilde L_{\text{C}}(t) = L_{\text{C}}(t)^\lambda, \quad 0 < \zeta < 1,\; 0 < \lambda < 1.\]

\textit{What is $\rho_X$?} The smaller $\rho_X$ is, the less it's possible to only increase one of $C_{\text{xpm}}(t)$ and $L_C(t)$ in order to increase $X(t)$.

\textit{What is $\alpha$?} The closer $0< \alpha < 1$ is to $1$, the more experiment compute is considered important relative to coding labor.

\textit{What is $\lambda$?} We use the smaller quantity $L_C(t)^\lambda$ instead of $L_C(t)$ since $L_C(t)$ is a measure of \textit{parallel} coding labor but we want a measure of \textit{serial} coding labor (a single programmer can make more progress in two days than a team of two programmers can make in a day because of the ``stepping on toes" effect). The reason we use serial (rather than parallel) coding labor in the CES function is that if the project had unlimited experiment compute, the way we double experiment throughput is by letting the coding workforce work for twice as long (or speeding up the coding workforce by $2 \times$), rather than doubling the size of the coding workforce.

\textit{What is $\zeta$?} The reason we discount experiment compute by the exponent \(0<\zeta<1\) is that, if one had unlimited coding labor, doubling experiment compute doesn't double experiment throughput because model size increases, meaning the ability to run near-frontier experiments goes down.

\ASUG{I wonder if discussion about how to set parameters should be put elsewhere.}

We set the parameter values as follows: \begin{enumerate}
    \item $\lambda$: We estimated this based on our intuitions about the effect of increased parallel labor on research effort, as well as interviews with frontier AI researchers.
    \item $\zeta, \alpha, \rho_X$: We estimated three quantities, from a fixed starting point of 2024, that together pin down the values of $\zeta$, $\alpha$, and $\rho_X$: (a) the decrease in $X(t)$ from a $10 \times$ decrease in $C_{\text{xpm}}$, (b) the $X(t)$ asymptote as we take $C_{\text{xpm}}$ to infinity, and (c) the $X(t)$ asymptote as we take $L_C(t)$ to infinity. To estimate these quantities, we combined frontier AI researcher interviews, surveys of other AI experts, and our own reasoning about how $X(t)$ would change given infinite coding labor or compute.
\end{enumerate}

\subsection{Software efficiency} \label{Software efficiency}

We next discuss how experiment throughput $X(t)$ translates into growth in software efficiency $S(t)$. As shown in our diagram representing the model, we have an intermediate variable between experiment throughput and software efficiency called ``software research effort", or ``research effort" for short. As we will elaborate on later, research effort $RE(t)$ is supposed to be a measure of \textit{quality-adjusted} experiment throughput:
\[ RE(t) = T(t) \, X(t),\]
where $T(t)$ is the \textit{aggregate experiment selection skill} of the project. If we ignore the contribution of AI automation, the aggregate experiment selection skill is just the average human experiment selection skill, which is normalized to 1. So in this case, research effort is the same as experiment throughput:
\[ RE(t) = T(t) \, X(t) = X(t) \qquad \textit{(assuming no modeling of R\&D automation)}.\]
For consistency with the explanations to come, we explain how changes in research effort $RE(t)$ (rather than experiment throughput $X(t)$) lead to changes in software efficiency $S(t)$, even though $RE(t)$ is the same as $X(t)$ when we don't consider AI contribution.

\textbf{Converting research effort gains into software efficiency gains}

Recall that software efficiency, $S(t)$, measures how efficiently the training process at time $t$ can convert training compute into performance. We have $S(\text{present-day}) = 1$, and $S(\text{June 2027}) = 1000$ would mean: if we used the training process of the present-day, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027.

We want to understand how $S(t)$ changes as research effort $RE(t)$ changes. Technically though, $S$ is not a function of $RE$ (which is a measure of how ``fast" the AI project is going), but of how much work has actually been done. So define the research stock, $RS(t)$, to be the \textit{total} amount of research effort done by time $t$; in symbols,
\[ RS(t) = \int_{-\infty}^t RE(\tau) \, d\tau.\]
So research stock is to research effort what distance traveled is to speed.

Both research stock and software efficiency describe ``how far" the AI project has come, but research stock (like research effort) is about effort/investment while software efficiency is about fruits/returns. We want to know their relationship:
\[ \text{$RS \approx$ total AI software R\&D work done} \quad \xrightarrow{\text{relationship?}} \quad \text{$S \approx$ total returns}\]

When we look at past empirical data, we see that $S(t)$ has been growing exponentially over time. Meanwhile research stock should also be growing exponentially, since both experiment compute and AI software R\&D labor have been growing exponentially over the last 10 years. So, since $S(t)$ and $RS(t)$ both grow exponentially over time (with different bases for the exponentials), the relationship between them is best modeled by a power law: 
\[ S(t) = C \cdot RS(t)^{1/\beta}, \quad \text{for some fixed parameters $C$ and $\beta > 0$}.\]
Intuitively:
\[ \text{$\beta$ doublings of research stock $RS$} \quad \xrightarrow{\text{produces}} \quad \text{1 doubling of software efficiency $S$}\]

\textit{Diminishing returns}: AI capabilities come from a combination of software efficiency $S$ and training compute $C_{\text{train}}$. Since training compute has been growing exponentially, for $S$ to ``keep up" and not fade into irrelevance, it also needs to be growing exponentially. So it's natural to talk about returns on research effort in terms of $\overline{S} = \log S$ (i.e. orders of magnitude of $S$). Since
\[ \overline{S} = \log C + \frac{1}{\beta} \cdot \log RS \approx \frac{1}{\beta} \cdot \log RS,\]
successive equal-sized increases in $RS$ lead to smaller and smaller increases in $\overline{S}$ over time. This is the phenomenon of ``diminishing returns": the more progress has already been made, the harder it becomes to make impactful new discoveries since the low-hanging fruits have been picked.

\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{diminishing_returns.png}
\end{figure}

\textbf{Expandable: Connection to the Jones semi-endogenous growth model}

Jones 1995 uses the following growth law to model idea production:
\[
A'(t) = a \, A(t)^{1-\beta} \, R(t)^\lambda.
\]
Here $A(t)$ is the stock of ideas and $R(t)$ is the number of researchers engaged in idea production. The parameter $\lambda > 0$ controls the ``stepping on toes'' effect from adding additional researchers, and $\beta>0$ represents the strength of the ``fishing out'' effect, according to which ideas become harder to find as the stock grows.

In past AGI takeoff models (e.g., \href{https://takeoffspeeds.com/description.html}{Davidson's FTM model} and \href{https://epoch.ai/gate}{Epoch's GATE model}), software efficiency $S(t)$ replaced the stock of ideas $A(t)$. All proposed models have had the form
\[S'(t) = a \, S(t)^{1-\beta} \, RE(t),\]
for various definitions of research effort $RE(t)$.

We show this growth law reduces to our formula $S(t) = C \cdot RS(t)^{1/\beta}$ from above. We have:
\begin{align*}
RS(t) &= \int_{-\infty}^t RE(\tau) \, d\tau = \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau. \\
\intertext{The integral on the right can be computed via the substitution $U = S(\tau)^\beta$, $dU = \frac{\beta S'(\tau)}{ S(\tau)^{1-\beta}}d\tau$:}
RS(t) &= \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau \\
 &= \frac{1}{a\beta} \left(S(t)^\beta - S(-\infty)^\beta\right) \\
 &= \frac{1}{a \beta} S(t)^\beta . \\
\intertext{Solving for $S(t)$,}
S(t) &= \left( a \beta \right)^{1/\beta} RS(t)^{1/\beta} = C \cdot RS(t)^{1/\beta}, \quad \quad \quad \text{for $C = \left(a\beta\right)^{1/\beta}$}. 
\end{align*}
\textbf{End of expandable}

\subsection{Effective compute}

While software efficiency $S(t)$ is a measure of how good the training process is, ultimately AI capabilities depend on training compute $C_{\text{train}}(t)$ in addition to $S(t)$. Recall that effective compute $E(t)$ is the amount of training compute we'd need to train models as performant as the frontier models at time $t$ using the training process of the present-day, and is given by:
\[ E(t) = C_\text{train}(t) \, S(t).\]
It's a useful measure of AI progress which captures the idea that scaling training compute would in principle result in arbitrarily high AI capabilities \ASUG{More motivation for compute-centric view?}

Effective compute is the key metric tracked by our model. As described earlier, we model time horizon as a function of effective compute (not calendar time) in order to account for changes in compute, human labor, and AI automation that wouldn't be explicit if time horizon was modeled as a function of time. And, as described in the next two sections, our model expresses AIs' capabilities to automate coding and research as functions of effective compute.

\end{comment}

\section{Coding automation} \label{Coding automation}

We now turn to describing how our model sets AIs' R\&D capabilities and how that affects the inputs to software research effort.

\subsection{Coding automation fraction} \label{Coding automation fraction}

We think coding is reasonably well described as a stable set of types of tasks, and that some tasks will be (or already are) automatable far before others. %Thus, we model coding automation using a task-based approach.
We define the \textit{coding automation fraction} $A(t)$ to be the fraction of coding work (involved in frontier AI research) that can be efficiently automated. For example, $A(\text{March 2029}) = 80\%$ means that we could use the AIs of March 2029 to automate away $80\%$ of the coding work done in the present-day and not lose in productivity ($80\%$ as measured by the proportion of clone-parity coders doing that work; see the expandable box below).

%\ASUG{A bit awkward to put such emphasis on the task-based approach in the first two sentences, then define $A(t)$ as the fraction of coding ``work" that's automatable, rather than the fraction of ``tasks" that's automatable.}

\label{When is a task considered efficiently automatable?}
\textbf{Expandable: When is a task considered efficiently automatable?} We define a ``clone-parity coder" by the condition that, if every human coder were replaced by a clone-parity coder, productivity would remain the same. For each type of coding task, we weight its importance by how many clone-parity coders would work on the task absent AI assistance. We consider a task efficiently automatable when AIs can complete the task faster than the human coders and using no more than $\frac{B}{N}$ of present-day automation compute, where $B$ is the number of human clone-parity-coder-equivalents working on the task in the present-day and $N$ is the total number of human coders in the present-day. \textbf{End of expandable}

Our definition of the Automated Coder milestone does not technically imply that $A(t_{\text{AC}}) = 1$ at the time $t_{\text{AC}}$ when the milestone is reached. Nevertheless, we will make the simplifying assumption that this is the case.

\textbf{Expandable: Why doesn't the definition of AC literally imply all tasks are efficiently automatable?} The Automated Coder (AC) milestone was defined by the condition that if the AC was dropped into the present day, it would overall be as productive as all present-day human coders with no AI assistance. This can be true even if it's still less efficient than humans for \textit{some} proportion of tasks. \textbf{End of expandable}

We model coding automation fraction $A(t)$ as a linear function of OOMs (orders of magnitude) of effective compute $\overline{E}(t) = \log E(t)$. That is, we estimate the automation fraction $A(\text{present-day})$ in the present, and let $A(t)$ vary linearly as a function of $\overline{E}(t)$ until $A(t)$ reaches 1 when $\overline{E}(t)$ equals the effective compute $\overline{E}_{\text{AC}}$ for the Automated Coder anchor. So:
\[ \text{$\overline{E}(t)$ is $y \%$ of the way between $\overline{E}(\text{present-day})$ and $\overline{E}_{\text{AC}}$}\]
\[\downarrow\]
\[\text{$A(t)$ is $y \%$ of the way between $A(\text{present-day})$ and $1$}.\]

\subsection{Coding automation efficiency} \label{Coding automation efficiency}

We use indices $i$ in the interval $[0,1]$ to represent the various coding tasks involved in implementing research experiments. At any given time $t$, tasks $i$ such that $0 \leq i \leq A(t)$ are considered efficiently automatable, while tasks $i$ such that $A(t) < i \leq 1$ are not.

For each task $i$, the coding automation efficiency $\eta_i(t)$ is the conversion rate between units of automation compute (i.e. H100-equivalents) and the equivalent in human coders. In other words, if $C_{\text{aut}, i}(t)$ is the amount of compute assigned to task $i$, then this corresponds to having $\eta_i(t) \, C_{\text{aut}, i}(t)$ additional human coders work on task $i$. So $\eta_i(t)$ is a measure of the runtime efficiency of AIs for task $i$; efficiency gains come from a combination of making AIs faster, being able to run more AIs, and getting more capable AIs that can complete tasks with fewer actions.

% To model how efficiently AIs can do coding tasks, we decided that once a task is efficiently automatable, the automation efficiency $\eta_i(t)$ should increase with effective compute in a power-law fashion.

Writing $E_i$ for the effective compute corresponding to when task $i$ is first efficiently automatable, our model for $\eta_i(t)$ is:
\[ \eta_i(t) = \begin{cases} \eta_{\text{init}} \cdot \left(\frac{E(t)}{E_i}\right)^{\eta_{\text{slope}}} & \text{if task $i$ is efficiently automatable by time $t$;}\\ 0 & \text{otherwise;} \end{cases}\]
where $\eta_{\text{init}}$ is a parameter representing the initial efficiency when a task first gets automatable, and $\eta_{\text{slope}}$ is a parameter controlling how quickly automated coders’ efficiency continues to improve afterward. This can be intuitively understood as:
\[ \text{each doubling of effective compute $E$}\]
\[\downarrow\]
\[\text{$\eta_{\text{slope}}$ doublings of coding automation efficiency.}\]
By \hyperref[When is a task considered efficiently automatable?]{our definition} of when a coding task is efficiently automatable, the initial conversion rate $\eta_{\text{init}}$ is the ratio of the total number of human coders to the amount of automation compute in the present-day.

This model of automation efficiency $\eta_i(t)$ makes the simplifying assumption that the relationship between effective compute and efficiency stays consistent across different efficiency levels, which is technically false. However, we think changing this would not substantially affect results.

\subsection{Aggregate coding labor} \label{Aggregate coding labor}

We next discuss how the increasing AI coding abilities feed back into \textit{aggregate coding labor}, which in turn influences experiment throughput. Recall that experiment throughput (i.e. the capacity of the project to implement and run experiments) was a function of coding labor and experiment compute, and coding labor $L_C(t)$ was described as ``the number of full-time-human-equivalents". We want $L_C(t)$ to account for both human coding labor and AI coding labor, so we now call it ``aggregate coding labor". The number of human coders will be denoted by $L_{C,H}(t)$ instead.

Recall that we use indices $i$ in the interval $[0,1]$ to represent the different coding tasks involved in implementing AI experiments. We write $L_{C,H,i}(t)$ and $C_{\text{aut},i}(t)$, respectively, for the amount of human coding labor and AI coding labor allocated to task $i$. The aggregate coding labor for task $i$ is
\[ G_i(t) = L_{C,H,i}(t) + \eta_i(t) \cdot C_{\text{aut},i}(t),\]
since $\eta_i(t)$ is the conversion rate between units of automation compute and human coders for task $i$.

If AIs cheaply automate some coding tasks but not others, what effects will that have on coding labor? This depends how much one can substitute the automatable tasks for the non-automatable tasks. But if $G_i(t)$ goes to infinity for, say, 90\% of the tasks, this shouldn't cause $L_C(t)$ to go to infinity since coding will then bottleneck on the remaining 10\% of tasks. We choose to model this using a task-based CES (``constant elasticity of substitution") production function with a parameter $\rho_C < 0$ that controls the level of substitutability. We are unsure about how well task-based CES functions model reality, and there are known issues with them [\ETODO{cite}], but they are the best tool we know of for this job (we'd be interested in work that more granularly models the coding workflow).

\textbf{Expandable: Explicit formula for $L_C(t)$.}
\[ L_C(t) = \left( \int_0^1 G_i(t)^{\rho_C} \, di \right)^{1/\rho_C} = \left(\int_0^1 (L_{C,H,i}(t) + \eta_i(t) \cdot C_{\text{aut}, i}(t))^{\rho_C} \, di \right)^{1/\rho_C}.\]
\textbf{End of expandable}

\textit{What is $\rho_C$?} The smaller the substitution parameter $\rho_C < 0$, the more coding bottlenecks on the tasks with little coding labor. Put differently, $\rho_C$ much smaller than 0 would mean coding AI experiments is a fairly rigid set of tasks that must always be done, and $\rho_C$ close to 0 would mean that one can more easily substitute easy-to-automate tasks for hard-to-automate tasks. %\ESUG{$\rho_c = -\infty$ would correspond to ``Amdahl's Law," i.e. no substitution between tasks, meaning that being able to accomplish 90\% of tasks infinitely would lead to a 10x increase in $L_C(t)$. As $\rho_c$ approaches 0, $L_C(t)$ would approach $\infty$} \ASUG{I think we should not mention this.}

When running simulations of our model, at each timestep we set the allocations $L_{C,H,i}(t)$ and $C_{\text{aut},i}(t)$ to maximize the aggregate coding labor $L_C(t)$ (subject to the constraints that the allocations $L_{C,H,i}$ sum to the total human coding labor $L_{C,H}(t)$ and the allocations $C_{\text{aut},i}(t)$ sum to the total automation compute $C_{\text{aut}}(t)$). This corresponds to the AI project allocating its human and AI coding workforce to where it's most useful; more concretely, the human coders will be reallocated to tasks that still evade automation while the AI coders will be allocated to the tasks they can efficiently automate (especially those tasks they can automate very quickly).

\section{Experiment selection automation}\label{Experiment selection automation}

\BTODO{Update this section given changes made re: algorithmic limits (with funny CES taste distribution).}

Experiment selection skill (called ``research taste" in our prior work) intuitively measures the value one gets per experiment. Doubling experiment selection skill would have the same effect on progress as doubling experiment throughput (that is, as doubling the number of experiments the project can implement per unit time). We take an expansive definition of experiment selection, including: high-level research direction setting, low-level choices, and experiment ideation and interpretation. 

In this section we explain our model of how AIs' experiment selection skill improves with effective compute and how the experiment selection skills of humans and AIs combine to give the aggregate experiment selection skill. 

We discussed how Stage 1 of our model between the present-day and the Automated Coder milestone is largely driven by extrapolating the METR time horizon trend (and deciding whether to extrapolate with an exponential, a superexponential, or a subexponential). Stage 2 of the model behavior, once the Automated Coder milestone is reached, is largely driven by how quickly the experiment selection skill of the AIs increases with effective compute, as well as the initial experiment selection skill at the Automated Coder milestone. The complete automation (and speedup) of coding also plays a notable role, but based on our model simulations the rate at which AIs improve at experiment selection is a bigger driver. Intuitively, additional coding labor can only help so much because the ability to run experiments will eventuall bottleneck on experiment compute. On the other hand, it's always helpful to have more skillful automated selection of experiments.

\subsection{Human experiment selection skill} \label{Human experiment selection skill}

Before we get to improvements in automated experiment selection, we should discuss our model of human experiment selection skill. We model the distribution of the experiment selection skills of human researchers with a lognormal distribution $T_H$ of mean 1 (``lognormal" means $\log (T_H)$ is a normal distribution). We choose the standard deviation of the lognormal so that the experiment selection skill of the $99.9^{\text{th}}$ percentile human researcher exceeds that of the median researcher by a factor of $T_{\text{var}}$. We conducted surveys of frontier AI researchers to inform the value of the parameter $T_{\text{var}}$.

\textbf{Expandable: Why use a lognormal distribution for experiment selection skill?} We choose a lognormal distribution because based on surveys of and discussions with frontier AI researchers, it seems likely that the top few percent of researchers provide much more value via experiment selection than typical researchers. Additionally, analogous quantities like scientific citation counts or startup success are often empirically lognormal. \textbf{End of expandable}

\subsection{Automated experiment selection skill} \label{Automated experiment selection skill}

As for coding automation fraction and efficiency, the automated experiment selection skill $T_{AI}(t)$ is modeled as a function of effective compute. Whereas we represent the human experiment selection skill by a lognormal distribution $T_H \sim \text{Lognormal}\left(\mu, \sigma^2 \right)$, $T_{AI}(t)$ is a single number for each time $t$ (we think of the AIs as a single collective entity, unlike for humans).

We describe the AIs' experiment selection skill in terms of how many standard deviations $\sigma$ (of the underlying normal of the human distribution) above or below the human median it is, and write $T_{AI}^{SD}(t)$ for this quantity. If $T_{AI}^{SD}(t)$ is negative, this means the AIs' experiment selection skill is below the human median; and if $T_{AI}^{SD}(t)$ is positive, this means the AIs' experiment selection skill is above the human median. We assume the AIs' experiment selection skill increases by $T_{\text{rate}}$ standard deviations for each additional OOM (order of magnitude) of effective compute:
\[ T_{AI}^{SD}(t) = T_{\text{AC}} + T_{\text{rate}} \left( \overline{E}(t) - \overline{E}_{\text{AC}} \right) \quad \text{in human standard deviations $\sigma$}.\]
Here $T_{\text{AC}}$ is a model parameter for the experiment selection skill of the Automated Coder, and $\overline{E}_{\text{AC}} = \log(E_{\text{AC}})$ is the effective compute corresponding to the Automated Coder.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{T_AI.png}
    \caption{$T_{AI}^{SD} = 2.5$ means the AIs' skill is 2.5 standard deviations above the human median \color{red}{(Need to make a new figure)}}
\end{figure}

\textbf{Expandable: Explicit formula for $T_{AI}(t)$}
Moving from standard deviations to absolute units, the AIs' experiment selection skill is
\[ T_{AI}(t) = \exp\left(\mu + \sigma \, T_{AI}^{SD}(t) \right),\] 
where $\mu$ and $\sigma$ were the parameters for the human experiment selection skill distribution $T_H \sim \text{Lognormal}\left(\mu, \sigma^2 \right)$.
\textbf{End of expandable}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Stage2.png}
\end{figure}

\subsection{Aggregate experiment selection skill} \label{Aggregate experiment selection skill}

The aggregate experiment selection skill $T(t)$ measures the average value per experiment, when one accounts for both human and AI experiment selection skill. When the AIs' experiment selection skill is better than that of the best human researcher, then $T(t)$ is just $T_{AI}(t)$. Before that's the case, our model assumes that each human researcher with lower skill than the AIs is lifted up to the level of the AIs. That is, the distribution of human experiment selection skill \textit{uplifted by the AIs} is
\[ \max \left(T_H, T_{AI}(t) \right).\]
The aggregate experiment selection skill (which is a single number rather than a distribution) is the average of this new distribution:
\[ T(t) = \mathbb{E} \left[ \max\left(T_H, T_{AI}(t)\right)\right].\]

We currently do not model the \textit{quantity} of labor needed to do experiment selection, and believe this may not drastically change results, since for experiment selection, quality matters much more than quantity.

\begin{comment}
\section{Closing the R\&D feedback loop} \label{Feedback loop recap}

We initially explained our way of modeling AI R\&D if we ignored the contribution of the AIs to coding and experiment selection. Experiment throughput $X(t)$ (i.e. the capacity of the project to implement and run experiments) was expressed as a ``CES function" of experiment compute $C_{\text{xpm}}(t)$ and coding labor $L_C(t)$. We initially presented $L_C(t)$ as the amount of human coding labor; in our actual model, $L_C(t)$ represents the aggregate coding labor. But the rest of the explanation of experiment throughput stays the same.

We also initially equated software research effort $RE(t)$ with experiment throughput $X(t)$, and then described the relationship between increases in $RE(t)$ (or ``research stock") and increases in software efficiency $S(t)$. When we account for the AIs' contribution to selecting experiments, $RE(t)$ and $X(t)$ come apart and are related by
\[ RE(t) = T(t) \, X(t).\]
As $T(t)$ increases, the average value of each experiment run also increases (relative to what it would have been with only human researchers).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Research_effort.png}
\end{figure}

As in the diagram, research effort gains then translates into software efficiency gains, which translate into effective compute gains, which translate into better AI coding and experiment selection skills, which increase aggregate coding and experiment selection skills, and which finally increase experiment throughput and research effort. And the feedback cycle resumes. This ``software recursive self-improvement" is a key feature of our model.
\end{comment}

\section{Zooming out again}

\ASUG{Maybe this would be a good place to talk about Stage 3 and whether or not we get a software intelligence explosion.}

\ATODO{I think Eli wants a recap section summarizing the whole model and with all the equations?}









\end{document}