<strong>AI Chip Lifespan</strong><br><br>

This parameter controls the rate at which AI chips fail over time. Empirical data on GPU failure rates is limited, but three sources inform our estimate:<br><br>

<strong><a href='https://arxiv.org/pdf/2407.21783' target='_blank' style='color: #6E7FD9;'>Meta's Llama 3 Training</a></strong><br>
Meta trained Llama 3 on 16,000 H100s over 21 million GPU-hours. Of 419 interruptions, only 3 required physical intervention—most failures were software-related.<br><br>

<strong><a href='https://cug.org/proceedings/cug2021_proceedings/includes/files/pap102s2-file1.pdf' target='_blank' style='color: #6E7FD9;'>Blue Waters (2013–2021)</a></strong><br>
This supercomputer operated 4,000 K20X GPUs under heavy use for 8 years. Approximately 97% survived the full period.<br><br>

<strong><a href='https://christian-engelmann.de/publications/ostrouchov20gpu.pdf' target='_blank' style='color: #6E7FD9;'>Titan (2012–2018)</a></strong><br>
Cooling significantly affects longevity. Among well-cooled GPUs, 60–90% survived 5 years; poorly-cooled units showed 10–40% survival rates.
