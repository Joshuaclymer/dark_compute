\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{fancyhdr}
\usepackage{amsfonts}
\usepackage{hyperref}
\usepackage{parskip}
\usepackage{titlesec}
\usepackage{graphicx}
\usepackage{float}
\usepackage{xcolor}
\usepackage{amsthm}
\usepackage{verbatim}
\theoremstyle{definition}
\newtheorem*{definition}{Definition}

% Define a new sectioning level: subsubsubsection
\titleclass{\subsubsubsection}{straight}[\subsubsection]

\newcounter{subsubsubsection}[subsubsection]
\renewcommand\thesubsubsubsection{\thesubsubsection.\arabic{subsubsubsection}}

\newcommand{\BTODO}[1]{\textcolor{brown}{BH-TODO: #1}}
\newcommand{\BSUG}[1]{\textcolor{orange}{BH-SUGGESTION: #1}}
\newcommand{\ETODO}[1]{\textcolor{blue}{ELI-TODO: #1}}
\newcommand{\ESUG}[1]{\textcolor{purple}{ELI-SUGGESTION: #1}}
\newcommand{\ENOTE}[1]{\textcolor{purple}{ELI-NOTE: #1}}
\newcommand{\ATODO}[1]{\textcolor{red}{ALEX-TODO: #1}}
\newcommand{\ASUG}[1]{\textcolor{magenta}{ALEX-SUGGESTION: #1}}
\newcommand{\ANOTE}[1]{\textcolor{magenta}{ALEX-NOTE: #1}}

\newcommand{\codinganchor}{coding automation anchor}

\titleformat{\subsubsubsection}
  {\normalfont\normalsize\bfseries}{\thesubsubsubsection}{1em}{}

\titlespacing*{\subsubsubsection}
  {0pt}{3.25ex plus 1ex minus .2ex}{1.5ex plus .2ex}

% Make sure LaTeX numbers and shows them in the TOC
\setcounter{secnumdepth}{4}
\setcounter{tocdepth}{4}

\pagestyle{fancy}
\fancyhf{}
\cfoot{\thepage}

\setlength{\belowdisplayskip}{1.5\baselineskip}
\setlength{\belowdisplayshortskip}{1.0\baselineskip}

%\title{Explanations to accompany the model diagram}
%\author{Alex Kastner, Eli Lifland}
%\date{\today}

\begin{document}

%\maketitle

\begin{comment}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Model_diagram.png}
    \label{fig:placeholder}
\end{figure}

\textit{The above diagram will appear prominently on the website. When users click on each variable in the diagram, they will be able to see definitions of what that variable is, and how it is expressed in terms of other variables. This doc contains drafts of the explanations people will see.}

\section{Symbols used}

\ENOTE{Is this section going to be on the site?}

\begin{itemize}
\item $L_{C,H}(t) =$ human coding labor
\item $C_\text{aut}(t) =$ automation compute
\item $C_\text{xpm}(t) =$ experiment compute
\item $T_H =$ human experiment selection skill
\item $C_\text{train}(t) =$ training compute
\item $L_C(t) =$ aggregate coding labor
\item $T(t) =$ aggregate experiment selection skill
\item $X(t) =$ experiment throughput
\item $RE(t) =$ software research effort
\item $S(t) =$ software efficiency
\item $E(t) =$ effective compute
\item $A(t) =$ coding automation fraction
\item $AC =$ automated coder milestone
\item $T_{AI}(t) =$ automated experiment selection skill
\end{itemize}
\end{comment}

%\ASUG{If this does appear as a list of explanations (rather than isolated pop-up explanations) then we may want to rearrange the order in which the variables are presented. For example, it would make sense to have the three types of compute presented in succession.}

\begin{comment}
\textbf{Eli orienting to page version:}

This website presents AI Futures Project's latest AI capabilities model, following up on the timelines and takeoff models we published alongside AI 2027. Our model tracks AIs' coding and experiment selection skills, and outputs the dates that important milestones are achieved. On this page you can see the effects of adjusting parameters and turning off model features, and you can learn about the model with an interactive diagram.
\end{comment}

At a high level, our model works as follows:
\begin{enumerate}
    \item The model tracks the evolution of \hyperref[Effective compute]{effective compute}, which is a capability measure that combines \hyperref[Training compute]{training compute} and \hyperref[Software efficiency]{software efficiency}.
    \item \hyperref[Effective compute]{Effective compute} is mapped onto how good AIs are at:
    \begin{enumerate}
    \item \hyperref[Coding automation fraction and efficiency]{coding} (shorthand for an expansive definition of experiment implementation), and
    \item \hyperref[Automated experiment selection skill]{experiment selection} (includes high-level direction-setting).
    \end{enumerate}
    \item These AI capabilities are combined with \hyperref[Human coding labor]{human coding labor} and \hyperref[Human experiment selection skill]{human experiment selection skill} to determine the \hyperref[Aggregate labor]{aggregate labor} involved in AI software R\&D.
    \item \hyperref[Aggregate labor]{Aggregate labor} is combined with \hyperref[Experiment compute]{experiment compute} to get \hyperref[Software research effort]{software research effort}, after which we take into account diminishing returns when calculating the resulting \hyperref[Software efficiency]{software efficiency} gain.
    \item Finally, \hyperref[Software efficiency]{software efficiency} is combined with \hyperref[Training compute]{training compute} to get \hyperref[Effective compute]{effective compute}. Then we loop back to (2).
\end{enumerate}

The sections below provide a more detailed version of the above: explanations of the variables used in the model, and of how each variable is expressed in terms of others. Clicking on a variable name in the above interactive diagram will bring you to the section explaining that variable. We encourage users to understand the model through the diagram, and decide which variables to click on by ``following the arrows". The explanation of each variable was written to be understandable in isolation. \textit{We think actively interacting with the diagram is a more efficient way to understand the model than reading the explanations from top to bottom.} Because the model has feedback cycles, the formulas for many variables depend on other variables that are more fully explained in later sections. So one probably needs to circle back at least a few times.

%\textbf{Alex version:}

% Introduction to the model: What is this a model of? What is the motivation behind it? Why should they care? 

%Thanks for checking out our new timelines and takeoff model! We believe our new unified model is more realistic, better documented, and better explained.

%Our model’s output is the trajectory of AIs’ ability to automate AI software R\&D, i.e. AIs’ ability to find software techniques that improve performance given the same amount of training compute. We assume there are two activities involved in AI software R\&D:
%\begin{enumerate}
%\item Experiment selection (includes high-level direction-setting);
%\item Coding (shorthand for an expansive definition of experiment implementation).
%\end{enumerate}
%Our model tracks how these two capabilities improve over time, and how these capabilities then feed into faster AI software progress. %We also track increases in training compute. Software progress and training compute increases are combined into effective compute.
\begin{comment}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{Experiment_loop.png}
    \caption{AI software progress comes from coding labor, experiment compute, and experiment selection skill.}
\end{figure}
\end{comment}

%What follows are explanations of the variables used in the model, and of how each variable is expressed in terms of others. The explanations are meant to accompany the above interactive diagram; clicking on a variable name will bring users to the section explaining that variable. We encourage users to try to understand the model through the diagram, and decide which variables to click on by ``following the arrows". The explanations of each variable were written to be understandable (roughly) in isolation. \textit{We think actively interacting with the diagram is a more efficient way to understand the model than reading the explanations from top to bottom.} And because the model has feedback cycles, the formulas for many variables necessarily depend on other variables that are more fully explained in later explanations. So one probably needs to circle back after exploring the whole model.

\section{Inputs} 

The quantities below are provided to the model as input time series, i.e. they progress the same way across time regardless of the rest of the model simulation (this is often referred to as the quantities being \textit{exogenous}).

\subsection{Human coding labor} \label{Human coding labor}

Human coding labor at time $t$, $L_{C,H}(t)$, is the number of full-time-equivalent human coders at time $t$ at the leading AGI company. The main task we have in mind for coders is the implementation of experiments for AI R\&D.

The $L_{C,H}(t)$ time series is forecasted by extrapolating time series data for the growth of the number of employees at frontier AI companies. \ETODO{Edit once we have an adjustment down, say that.}

\textbf{Expandable: Accounting for coders of different skill levels}

Since our model converts AI coding labor into human-coder equivalents, and not all humans have the same level of coding skill, formally $L_{C,H}(t)$ will be expressed in terms of what we call ``clone-parity coders". This is an operationalization of a typical human coder: if every human coder at time $t$ were replaced with a clone-parity coder, productivity remains unchanged.

One could also imagine normalizing based on wages.

For simplicity our current input time series does not attempt to adjust for quality. \textbf{End of expandable}

\subsection{Automation compute} \label{Automation compute}

Automation compute at time $t$, $C_{\text{aut}}(t)$, measures the compute used for running coding agents that help with experiment implementation. These coding agents can either augment human engineers or replace them entirely.

$C_{\text{aut}}(t)$ is estimated by forecasting time series data for compute growth at frontier AI companies, then estimating that coding automation compute accounts for 5\% of this total. This is likely an overestimate of today's automation compute, but $5\%$ is our best guess for the automation compute fraction as we approach and attain full coding automation, when automated coders matter a lot more than they do today.

\subsection{Experiment compute} \label{Experiment compute}

Experiment compute at time $t$, $C_{\text{xpm}}(t)$, measures the compute used for research experiments.

$C_{\text{xpm}}(t)$ is estimated by forecasting time series data for the compute growth at frontier AI companies, then estimating that experiment compute accounts for 50\% of this total from present day onward, while covering as much as 75\% in the past.

%\ASUG{(Josh feedback) Maybe should clarify the above sentence. Does this mean our model uses a changing fraction for fraction of total compute allocated to experiments?}

\subsection{Human experiment selection skill} \label{Human experiment selection skill}

\BTODO{Rewrite this with funny CES taste distribution.}

Experiment selection skill (called ``research taste" in our prior work) intuitively measures the value one gets per experiment. Doubling experiment selection skill while holding experiment compute and coding labor fixed would double the pace of progress. We take an expansive definition of experiment selection, including: high-level research direction setting, low-level choices, and experiment ideation and interpretation. 

We model the distribution $T_H$ of human researcher experiment selection skill with a lognormal distribution of mean 1. We choose the standard deviation of the lognormal so that the experiment selection skill of the $99.9^{\text{th}}$ percentile human researcher exceeds that of the median researcher by a factor of $T_{\text{var}}$. We conducted surveys of frontier AI researchers to inform the value of $T_{\text{var}}$.

\ASUG{Would be nice to have a figure to illustrate the lognormal distribution for human experiment selection skill}

\textbf{Expandable: Why use a lognormal distribution for experiment selection skill?} We choose a lognormal distribution because based on surveys of and discussions with frontier AI researchers, it seems likely that the top few percent of researchers provide much more value via experiment selection than typical researchers. Additionally, analogous quantities like scientific citation counts or startup success are often empirically lognormal. \textbf{End of expandable}

\subsection{Training compute} \label{Training compute}

Training compute, $C_{\text{train}}(t)$, measures the compute used to train the most capable model available at time $t$. The units of $C_{\text{train}}(t)$ are 16-bit floating-point operations (FLOP).

$C_{\text{train}}(t)$ is estimated by extrapolating past trends, then predicting a slowdown in the late 2020s due to hitting investment limits.

\textbf{Expandable: How we forecast AGI company compute growth}

We forecast training compute using X, experiment using Y, etc. \BTODO{Add more} \textbf{End of expandable}

\section{Aggregate Labor} \label{Aggregate labor}

\subsection{Aggregate coding labor} \label{Aggregate coding labor}

Aggregate coding labor, $L_C(t)$, is a measure of the total coding labor accounting for both human coders and AI coders (measured in human-coder-equivalents). 

We use indices $i$ in the interval $[0,1]$ to represent the different coding tasks involved in implementing AI experiments. The AI project allocates human coding labor $L_{C,H,i}(t)$ and AI coding labor $C_{\text{aut},i}(t)$ to task $i$. The aggregate coding labor for task $i$ is
\[ G_i(t) = L_{C,H,i}(t) + \eta_i(t) \cdot C_{\text{aut},i}(t),\]
where $\eta_i(t)$ is the conversion rate between units of automation compute and human coders for task $i$ (see \hyperref[Coding automation efficiency]{Coding Automation Efficiency} for details about $\eta_i(t)$).

If one increases coding labor only in certain types of coding tasks but not others, then the coding will bottleneck on those more neglected tasks. This type of situation comes up often in economics, and is commonly modeled with a CES (``constant elasticity of substitution") production function with substitution parameter $\rho_C < 0$: 
\[ L_C(t) = \left( \int_0^1 G_i(t)^{\rho_C} \, di \right)^{1/\rho_C} = \left(\int_0^1 (L_{C,H,i}(t) + \eta_i(t) \cdot C_{\text{aut}, i}(t))^{\rho_C} \, di \right)^{1/\rho_C}.\]
See \hyperref[Experiment throughput]{Experiment Throughput} for a bit more explanation about CES production functions.

\textit{What is $\rho_C$?} The smaller the substitution parameter $\rho_C < 0$, the more coding bottlenecks on the tasks with little coding labor. Put differently, $\rho_C$ much smaller than 0 would mean coding AI experiments is a fairly rigid set of tasks that must always be done, and $\rho_C$ close to 0 would mean that one can more easily substitute easy-to-automate tasks for hard-to-automate tasks.

When running simulations of our model, at each timestep we set $L_{C,H,i}(t)$ and $C_{\text{aut},i}(t)$ to maximize the aggregate coding labor $L_C(t)$ (subject to the constraints that the allocations $L_{C,H,i}$ sum to the total human coding labor $L_{C,H}(t)$ and the allocations $C_{\text{aut},i}(t)$ sum to the total automation compute $C_{\text{aut}}(t)$). This corresponds to the AI project allocating its human and AI coding workforce to where it's most useful; more concretely, the human coders will be reallocated to tasks that still evade automation while the AI coders will be allocated to the tasks they can efficiently automate (especially those tasks they can automate very quickly).

\subsection{Aggregate experiment selection skill} \label{Aggregate experiment selection skill}

Aggregate experiment selection skill, $T(t)$, is the experiment selection skill (roughly, value per experiment) that results from combining that of AIs and humans. $T_H$ is a lognormal that models human experiment selection skill, while $T_{AI}(t)$ represents the AIs' experiment selection skill at time $t$ ($T_H$ is a distribution but $T_{AI}(t)$ is just a number). We assume that once a human has a lower skill level than the AIs, the AIs replaces them, i.e. for each human their skill is replaced with
\[ \max \left(T_H, T_{AI}(t) \right).\]
The aggregate experiment selection skill at time $t$ is the average of this new distribution:
\[ T(t) = \mathbb{E} \left[ \max\left(T_H, T_{AI}(t)\right)\right].\]

We currently do not model the labor needed to do experiment selection, and believe this likely doesn't drastically change results since for experiment selection, quality matters much more than quantity.

\section{Software R\&D}

\subsection{Experiment throughput} \label{Experiment throughput}

Experiment throughput, $X(t)$, measures the AI project's capacity for implementing and running experiments, and can roughly be understood as the number of experiments the project can run per unit time, where experiments are weighted by how much compute they use and how labor-intensive they are to code. It corresponds to how fast the AI project is going (i.e. \hyperref[Software research effort]{research effort $RE(t)$}) if we hold the aggregate experiment selection skill $T(t)$ fixed.

Experiment throughput goes up with both experiment compute $C_{\text{xpm}}(t)$ and coding labor $L_C(t)$, but there are diminishing returns to only increasing one of the inputs and not the other. In particular, if we take one of $C_{\text{xpm}}(t)$ and $L_C(t)$ to infinity but hold the other input fixed, then experiment throughput bottlenecks on the other input and should remain bounded. Indeed, if you have infinite coding labor but only a fixed amount of experiment compute, you can only implement so many experiments per unit time; and if you have infinite experiment compute but only a fixed number of coders, \ESUG{you could code something up to search the space of all possible AI designs, but that takes time and thus does not provide an infinite speedup.} %\ASUG{Maybe use a footnote for Eli's suggestion}
you don't have enough workers to code up experiments that (usefully) use the compute. This situation comes up in economics, where we would say that experiment compute and coding labor are complementary inputs that are hard to substitute for each other. It's commonly modeled with a CES (``constant elasticity of substitution") production function with substitution parameter $\rho_X < 0$:
\[ X(t) = \left( \alpha \, \tilde{C}_{\text{xpm}}(t)^{\rho_X} + (1-\alpha) \, \tilde{L}_C(t)^{\rho_X} \right)^{1/\rho_X}, \quad 0 < \alpha < 1,\; \rho_X < 0,\]
where
\[\tilde C_{\text{xpm}}(t) = C_{\text{xpm}}(t)^\zeta, \quad \tilde L_{\text{C}}(t) = L_{\text{C}}(t)^\lambda, \quad 0 < \zeta < 1,\; 0 < \lambda < 1.\]

\textit{What is $\rho_X$?} The smaller $\rho_X$ is, the less it's possible to only increase one of $C_{\text{xpm}}(t)$ and $L_C(t)$ in order to increase $X(t)$.

\textit{What is $\alpha$?} The closer $0< \alpha < 1$ is to $1$, the more experiment compute is considered important relative to coding labor.

\textit{What is $\lambda$?} We use the smaller quantity $L_C(t)^\lambda$ instead of $L_C(t)$ since $L_C(t)$ is a measure of \textit{parallel} coding labor but we want a measure of \textit{serial} coding labor (a single programmer can make more progress in two days than a team of two programmers can make in a day because of the ``stepping on toes" effect). The reason we use serial (rather than parallel) coding labor in the CES function is that if the project had unlimited experiment compute, the way we double experiment throughput is by letting the coding workforce work for twice as long (or speeding up the coding workforce by $2 \times$), rather than doubling the size of the coding workforce.

\textit{What is $\zeta$?} The reason we discount experiment compute by the exponent \(0<\zeta<1\) is that, if one had unlimited coding labor, doubling experiment compute doesn't double experiment throughput because model size increases, meaning the ability to run near-frontier experiments goes down.

We set the parameter values as follows: \begin{enumerate}
    \item $\lambda$: We estimated this based on our intuitions about the effect of increased parallel labor on research effort, as well as interviews with frontier AI researchers.
    \item $\zeta, \alpha, \rho_X$: We estimated three quantities, from a fixed starting point of 2024, that together pin down the values of $\zeta$, $\alpha$, and $\rho_X$: (a) the decrease in $X(t)$ from a $10 \times$ decrease in $C_{\text{xpm}}$, (b) the $X(t)$ asymptote as we take $C_{\text{xpm}}$ to infinity, and (c) the $X(t)$ asymptote as we take $L_C(t)$ to infinity. To estimate these quantities, we combined frontier AI researcher interviews, surveys of other AI experts, and our own reasoning about how $X(t)$ would change given infinite coding labor or compute.
\end{enumerate}

\subsection{Software research effort} \label{Software research effort}

% Maybe describe research effort as the number of quality-adjusted experiments you can run per unit time.
Research effort, $RE(t)$, is a measure of how fast the AI project is going. It is the product of experiment throughput $X(t)$ and the aggregate experiment selection skill $T(t)$:
\[ RE(t) = T(t) \, X(t).\]

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{Research_effort.png}
    \caption{Experiment throughput and experiment selection skill combine to give software research effort.}
\end{figure}

Doubling research effort means you get to any target capability in half the time. But research effort does not directly describe the fruits or returns of the research, that is, how quickly software efficiency improves. See \hyperref[Software efficiency]{Software Efficiency} for a description of how our model converts research effort into increased software efficiency.

Related to research effort, we track ``AI software R\&D uplift" (called ``AI R\&D progress multiplier" in our previous work), which measures how much faster AI software work is advancing with AI usage compared with without AI usage. More precisely, AI software R\&D uplift is the ratio of research effort to what research effort would be if we removed the AIs.\ESUG{Add footnote saying: Technically, the metric is what this ratio would be if the AI were transported into present day, not what the ratio is when it is developed.}

%\ANOTE{I need to better understand the definition of AI software R\&D uplift}

\section{Effective Compute}

\subsection{Software efficiency} \label{Software efficiency}

Software efficiency, $S(t)$, measures how efficiently the training process at time $t$ can convert training compute into performance. We have $S(\text{baseline-date}) = 1$ for some arbitrary choice of baseline-date (our baseline-date is Feb 17, 2025, when Grok 3 was released). As an example, $S(\text{June 2027}) = 1000$ would mean: if we used the training process of the baseline-date Feb 2025, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027.

%\ASUG{Maybe have a figure.}

We want to understand how $S(t)$ changes as the research effort $RE(t)$ changes. Technically though, $S$ is not a function of $RE$ (i.e. how fast the AI project is going), but of how much work has actually been done. So define the research stock, $RS(t)$, to be the \textit{total} amount of research effort done by time $t$; in symbols,
\[ RS(t) = \int_{-\infty}^t RE(\tau) \, d\tau.\]
So research stock is to research effort what distance traveled is to speed.

Both research stock and software efficiency describe ``how far" the AI project has come, but research stock (like RE) is about effort/investment while software efficiency is about fruits/returns. We want to know their relationship:
\[ \text{$RS \approx$ total AI software R\&D work done} \quad \xrightarrow{\text{relationship?}} \quad \text{$S \approx$ total returns}\]

When we look at past empirical data [cite?], we see that $S(t)$ has been growing exponentially over time. Meanwhile research stock should also be growing exponentially, since both experiment compute and AI software R\&D labor have been growing exponentially over the last 10 years.

So, since $S(t)$ and $RS(t)$ both grow exponentially over time (with different bases for the exponentials), the relationship between them is best modeled by a power law: %\ANOTE{Should we use parameters $C$ and $r$, or $a$ and $\beta$?}
\[ S(t) = C \cdot RS(t)^{1/\beta}, \quad \text{for some fixed parameters $C$ and $\beta > 0$}.\]
% Need to explain why we write $\beta$ instead of $r$. That's a downside of using $\beta$.
Intuitively:
\[ \text{$\beta$ doublings of research stock $RS$} \quad \xrightarrow{\text{produces}} \quad \text{1 doubling of software efficiency $S$}\]

\textit{Note about diminishing returns}: AI capabilities come from a combination of software efficiency $S$ and training compute $C_{\text{train}}$. Since training compute has been growing exponentially, for $S$ to ``keep up" and not fade into irrelevance, it also needs to be growing exponentially. So it's natural to talk about returns on research effort in terms of $\overline{S} = \log S$ (i.e. orders of magnitude of $S$). Since
\[ \overline{S} \approx \frac{1}{\beta} \cdot \log RS,\]
successive equal-sized increases in $RS$ lead to smaller and smaller increases in $\overline{S}$ over time. This is the phenomenon of ``diminishing returns": the more progress has already been done, the harder it becomes to make impactful new discoveries since the low-hanging fruits have been picked.

\begin{figure}[H]
    \centering
     \includegraphics[width=\linewidth]{diminishing_returns.png}
    \caption{Increases in research stock have diminishing returns for $\overline{S}$.}
\end{figure}

%\ASUG{Have a software intelligence explosion expandable. Maybe have the ``RE and S loop" Eli drew}


\textbf{Expandable: Alternative version of the Software Efficiency section. Which version do readers prefer?}
Software efficiency, $S(t)$, measures how efficiently the training process at time $t$ can convert training compute into performance. We have $S(\text{baseline-date}) = 1$ for some arbitrary choice of baseline-date (our baseline-date is Feb 17, 2025, when Grok 3 was released). As an example, $S(\text{June 2027}) = 1000$ would mean: if we used the training process of the baseline-date Feb 2025, it would take $1000 \times$ more compute than in June 2027 to train models as performant as the best from June 2027.

We now need to set the relationship between the software research effort $RE$ and software efficiency $S$. $RE$ is a function of experiment compute and AI software R\&D labor, both of which have been growing roughly exponentially over the last 10 years. Meanwhile, according to our estimates, $S$ has also been growing exponentially [ideally link to said estimates].

If we define research stock $RS(t)$ as the sum of all research efforts $RE(t)$ up until time $t$, this observation implies the following relationship between $RS$ and $S$:
\[ RS(t) = \int_{-\infty}^t RE(\tau) \, d\tau\text{; $RS$ is to $RE$ what distance traveled is to speed.}\]
\[ \text{$RS$ growing exponentially} \quad \xrightarrow{\text{implies}} \quad \text{ $S$ growing exponentially}\]

In order to keep up with growth in training compute $C_{train}$, $S$ needs to grow exponentially. If $S$ grows subexponentially, then software efficiency improvements will fade into irrelevance relative to training compute.

But in order to sustain exponential growth in $S$, $RE$ must also grow exponentially. Experiment compute $C_{xpm}$ should grow at a similar rate to training compute, so the question of whether $S$ can continue to "keep up" with $C_{train}$ depends on whether human and automated labor can continue at least exponential growth.

\[ \text{exponential increases in AI R\&D labor} \quad \xrightarrow{\text{are required for}} \quad \text{exponential growth in $S$ to keep up with $C_{train}$}\]

This can be explained by appealing to ``diminishing returns" to $RE$: as the ``low-hanging fruit" of software improvements get plucked, sustaining exponential growth of $S$ requires increasing amounts of effort. This phenomenon has sometimes been referred to as ``ideas getting harder to find`` [cite bloom].

%\ENOTE{It's a bit awkward that the below paragraph is so far from the one above, maybe should fix. Maybe just move this higher up? But then a bit awkward to talk for a while about stuff that doesn't directly explain our model.}

Recall that as $RS$ grows exponentially, $S$ should as well. In other words, this is a power law relationship. We model this via a parameter $\beta$ which enforces the following behavior:
\[ S(t) = C \, RS(t)^{1/\beta}.\]
Intuitively:
\[ \text{$\beta$ doublings of research stock $RS$} \quad \xrightarrow{\text{produces}} \quad \text{$1$ doubling of software efficiency $S$}\]
\textbf{End of expandable}

%\ESUG{I think it might be good to have figures which have time on the x axis and plot RE, RS, and S all on the same plot. To illustrate some of the above relationships. Or just one of RE/RS + S if all 3 at the same time is confusing.}

%\ASUG{Various possibilities: (a) Plot log S and log RS w.r.t. time, so that they both show up as lines with different slopes; (b) Plot log S w.r.t log RS; (c) Plot log S w.r.t. RS (similar to my picture above); (d) Plot log S and RS w.r.t. time (also shows ``diminishing returns").}

%Expand the below to get a more detailed mathematical description of how $S$ is determined in our model.

\textbf{Expandable: Connection to the Jones semi-endogenous growth model}

Jones 1995 uses the following growth law to model idea production:
\[
A'(t) = a \, A(t)^{1-\beta} \, R(t)^\lambda.
\]
Here $A(t)$ is the stock of ideas and $R(t)$ is the number of researchers engaged in idea production. The parameter $\lambda > 0$ controls the ``stepping on toes'' effect from adding additional researchers, and $\beta>0$ represents the strength of the ``fishing out'' effect, according to which ideas become harder to find as the stock grows.

In past AGI takeoff models [cite Davidson and GATE], software efficiency $S(t)$ replaced the stock of ideas $A(t)$. All proposed models have had the form
\[S'(t) = a \, S(t)^{1-\beta} \, RE(t),\]
for various definitions of research effort $RE(t)$.

We show this growth law reduces to our formula $S(t) = C \cdot RS(t)^{1/\beta}$ from above. We have:
\begin{align*}
RS(t) &= \int_{-\infty}^t RE(\tau) \, d\tau = \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau. \\
\intertext{The integral on the right can be computed via the substitution $U = S(\tau)^\beta, dU = \frac{\beta S'(\tau)}{ S(\tau)^{1-\beta}}d\tau$:}
RS(t) &= \frac{1}{a}\int_{-\infty}^t \frac{S'(\tau)}{S(\tau)^{1-\beta}}d\tau \\
 &= \frac{1}{a\beta} \left(S(t)^\beta - S(-\infty)^\beta\right) \\
 &= \frac{1}{a \beta} S(t)^\beta . \\
\intertext{Solving for $S(t)$,}
S(t) &= \left( a \beta \right)^{1/\beta} RS(t)^{1/\beta} = C \cdot RS(t)^{1/\beta}, \quad \quad \quad \text{for $C = \left(a\beta\right)^{1/\beta}$}. 
\end{align*}
\textbf{End of expandable}

\subsection{Effective compute} \label{Effective compute}

Effective compute, $E(t)$, is the product of training compute $C_\text{train}(t)$ and software efficiency $S(t)$:
\[ E(t) = C_{\text{train}}(t) \, S(t).\]
It's a convenient capability measure that lets us attribute progress to both compute scaling and software improvements. The coding and experiment selection capabilities of AIs are expressed as functions of effective compute.

%\ASUG{Probably Eli or Brendan should write the expandable below since they've thought about potentially using ECI.}

\textbf{Expandable: Why we use effective compute despite its limitations}

\BTODO{Write this (or EL if he has time)}

\textbf{End of expandable}

\section{AI capabilities}

\subsection{Automated coder (via time horizon)} \label{Automated coder}

%\ANOTE{I guess there are two more parameters I didn't make explicit in the model diagram: $H(t_p)$ and $E(t_p)$}

An ``automated coder" (or AC) is a specific capability milestone that influences the progression of the \hyperref[Coding automation fraction]{coding automation fraction}, the \hyperref[Coding automation efficiency]{coding automation efficiency}, and the \hyperref[Automated experiment selection skill]{automated experiment selection skill}. An AC is defined as a collective of AIs that, if dropped into the present day, would be as productive on their own as all present human coders with no AI assistance.

We forecast the arrival date of AC by considering time horizons, which were introduced by METR in [cite METR study] to measure AIs' coding ability. The coding time horizon, $H(t)$, is the length of the coding tasks the frontier model can successfully complete 80\% of the time, as measured by how long it takes humans to complete them. For example, if an AI has a 2 hour 80\% time horizon, that means it can complete with an 80\% success rate tasks that take human baseliners 2 hours (and for tasks that take baseliners more time, it has a lower success rate). \ESUG{We should add a figure illustrating the previous sentence, one of METR's sigmoid figures} We define our time horizon based on an imagined extension of the METR-HRS dataset, which we name METR-HRS-Extended. We use METR's time horizon benchmark because (a) it’s a coding benchmark (b) that has an established trend from effective compute to capabilities and (c) the trend can at least in theory be extrapolated to very high levels of performance.

\textbf{Expandable: Defining our theoretical extended benchmark METR-HRS-Extended.} Since METR-HRS only contains tasks that take human baseliners $\leq$ 30 hours to complete and we want to forecast up to much higher levels, we define a procedure that could in principle be used to extend the task suite even though it would not be practical to actually do so. It’s important to define which human baseliners are used during the extension: the more competent they are, the larger an impact we should expect at any given time horizon. We assume that METR finds human baseliners who take the same amount of time to complete tasks as a typical AGI company programmer who does similar tasks. We think that this might be loosely consistent with their past baselining, and this also represents a simple assumption that translates relatively well to real-world impacts. \textbf{End of expandable}

We focus on 80\% time horizons, though METR also reports 50\%, because we think higher reliability levels are more relevant. We also require AIs to achieve tasks efficiently in order for it to count as a success. In particular, we impose a constraint that the AI must accomplish tasks as efficiently as human researchers (see \hyperref[Coding automation fraction]{Coding automation fraction} for more details on what we mean by ``efficient automation").

\textbf{Expandable: Why only 80\% reliability?} At the moment, we only support reliability levels of 80\% since this is the highest reliability data that METR considers high-quality. How is it possible that an 80\% time horizon could correspond to dominating the average coder across all categories of tasks? (a) There is some randomness in whether an AI succeeds at an individual task, so even an AI that dominates average humans across all categories will not have 100\% reliability at human-level or above efficiency on individual tasks. (b) We adjust the required 80\% time horizon upward to account for increased reliability requirements. \textbf{End of expandable}

\textbf{Expandable: Why the efficiency threshold?} The METR paper does not set an efficiency threshold, but we consider it necessary to set some sort of efficiency threshold, given that AIs already sometimes accomplish tasks less efficiently than humans, and we expect this to become more important over time as AIs are better at translating more resources into better performance. \textbf{End of expandable}

Although the original METR study emphasized the evolution of time horizon as a function of time $t$, we want to express time horizon as a function of effective compute. The benefit of doing so is that effective compute, unlike time, is a quantity that tracks the evolution of inputs to AI progress as well as the automation feedback loop. So while the ``time horizon vs time" trend that METR observed may stop holding in the future, our assumption is that the ``time horizon vs effective compute" trend will keep holding, so we can track time horizon by modeling effective compute.

Both time horizon and effective compute increased roughly exponentially over the last 5 years, which suggests time horizon might grow exponentially as a function of OOMs (orders of magnitude) of effective compute. Writing $\overline{E}(t) = \log(E(t))$ for OOMs of effective compute and $t_p$ for the present-day, a first pass is to express $H(t)$ as:
\[ H(t) = H(t_p) \cdot 2^{\frac{\overline{E}(t)-\overline{E}(t_p)} {\tau}} \qquad \textit{(modified below)}\]
The parameter $\tau$ indicates the number of extra OOMs of effective compute needed to double time horizon:
\[ \text{effective compute $\overline{E}$ increases by $\tau$ OOMS} \quad \xrightarrow{\text{produces}} \quad \text{1 doubling of time horizon $H$}\]
We primarily set $\tau$ to fit historical METR-HRS trends, balancing between the long-term observed trend and a potential recent uptick.


In order to accommodate potential changes in the rate of time horizon growth, we allow the relationship between time horizon and effective compute OOMS to be faster or slower than exponential. We might expect superexponential growth because of the ``long-horizon agency argument": as an AI learns to complete increasingly long tasks, it may actually develop better long-horizon agency skills (long-term planning, decomposing long tasks, noticing and correcting mistakes, etc.) than the METR-HRS-Extended human baseliners. In fact, our best guess is that the time horizon vs OOMs of effective compute trend will be superexponential (see [cite Eli's explanation in the google doc] for a defense of this).

\textbf{Expandable: When AIs develop better long-horizon agency skills than humans, their success rates may not decrease even as tasks get longer.} 
\ANOTE{Should label the $y$-axis with ``Task success rate", and maybe make it clearer that the $x$-axis is the time it takes humans to \textit{complete tasks}.}
\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{infinite_horizons.png}
%    \caption{Three possibilites for once AIs have far surpassed humans at long-horizon agency.}
\end{figure}
Suppose we get AIs with better long-horizon agency skills than the METR-HRS-Extended human baseliners. If we plot the AIs' success rate on tasks vs. the time it takes the human baseliners to complete those tasks (the ``human-task-time"), what does that look like? There are three possibilities:
\begin{enumerate}
\item \textbf{The success rate vs. human-task-time trend ``flips" to AIs having higher success rates at higher human-task-times, leading to an infinite 80\% time horizon}. This could happen because AIs are now differentially better at long-horizon tasks than humans, as opposed to short-horizon tasks (recall that time horizons are defined relative to humans, as opposed to some inherent property of the task). In this case, we think this should be interpreted as the METR-HRS-Extended time horizon being infinity in finite effective compute and that therefore we should expect superexponential time horizon growth leading up to this.
\item \textbf{The success rate decreases with human-task-time, but it asymptotes above 80\%, leading to an infinite 80\% time horizon.} This could happen because, despite having better long-horion skills than the human baseliners, the AIs may have even better short-horizon skills.
\item \textbf{The success rate drops below 80\% but for extremely long human-task-times.} In this case, the 80\% time horizon vs effective compute trend could be either exponential or superexponential, but there wouldn't be a level of effective compute where the AIs have infinite 80\% time horizon.\ESUG{The fact that the time horizons are extremely large suggests that it would still most likely be superexponential, based on some back-of-the-envelope calcluations. (Include footnote that describes the calculations that I described in the GDoc)}
\end{enumerate}
\textbf{End of expandable}

On the other hand, one possible reason to expect subexponential growth, at least in the near-term, is data bottlenecks: it may be hard to collect high-quality data for long-horizon tasks. This could instead be modeled as having data as an explicit input to effective compute or AI progress, but our model doesn’t do this so forecasting subexponential time horizon growth is one possible way to compensate.

We model these possibilities with a doubling difficulty growth factor parameter $d > 0$: each time horizon doubling should take $d$ times what the last time horizon doubling took (in OOMs of effective compute). So $d=1$ corresponds to the exponential case, $0 < d < 1$ to the superexponential case, and $d > 1$ to the subexponential case.

\textbf{Expandable: Explicit formula for $H(t)$ with doubling difficulty.}
Starting from $H(t_p)$, the first doubling of time horizon requires $\overline{E}$ to increase by $\tau$, the second doubling by $d \tau$, the third doubling by $d^2 \tau$, and so forth. Provided $H(t)$ is not infinite yet (which eventually happens for $d < 1$), we can calculate an explicit formula for $H(t)$:
\[ H(t) = 
\begin{cases} 
H(t_p) \cdot 2^{\frac{\overline{E}(t)-\overline{E}(t_p)}{\tau}} & \text{if $d = 1$}; \\
H(t_p) \cdot \left(1 - (1-d) \frac{\overline{E}(t)-\overline{E}(t_p)}{\tau}\right)^{\log_d 2} & \text{if $d \neq 1$.}
\end{cases}
\]
\textbf{End of expandable}

\textbf{Expandable: Infinite time horizon when time horizon growth is superexponential.} If $0 < d < 1$, i.e. time horizon growth is superexponential, we get an infinite time horizon when
\[ \overline{E} = \overline{E}(t_p) + \frac{\tau}{1-d}.\]
As discussed previously, an infinite time horizon implies that AIs are more competent at long-horizon agency than human baseliners, \textit{not} that they have reached the limits of intelligence. \textbf{End of expandable}

\ESUG{Have a simple figure showing how $d$ works.}

%\ASUG{What time horizon did we choose for the AC anchor?}
%\ENOTE{There is a very wide uncetainty range. My current median is 100 years in the case of no gap. Btw you can see any of my median parameter estimates by looking at the Railway defaults or the parameter estimates + rationales portion of the GDoc:} \href{https://docs.google.com/document/d/1wsS2U4IG6k3C3wOzbNvzsljRuoaX_MqRG2NNZQEmER4/edit?tab=t.0#heading=h.e2jnqw2mwh56}{here}

We choose a specific 80\% time horizon, $H_{AC}$, to determine when Automated Coder is achieved, and write $\overline{E}_{AC}$ for the corresponding effective compute (in OOMs = orders of magnitude). We set $H_{AC}$ by estimating the time horizon of tasks that AI company programmers do, adjusting for a $>80\%$ success rate being required, and then adjusting for the difference between the METR-HRS-Extended benchmark and real-world coding tasks.

\textbf{Expandable: Explicit formula for $\overline{E}_{AC}$ without an effective compute gap.} Rearranging our formula expressing time horizon in terms of OOMs of effective compute gives:
\[     \overline{E}_{AC}=
    \begin{cases}
        \overline{E}(t_p) + \frac{\tau}{d-1}\left(\left(\frac{H_{AC}}{H(t_p)}\right)^{\log_2 d}-1\right) & \text{ if } d \neq 1; \\
        \overline{E}(t_p) + \tau \,\log_2\!\left(\frac{H_{AC}}{H(t_p)}\right) & \text{ if } d=1.
    \end{cases} \qquad \textit{(modified below)}\]
\textbf{End of expandable}

Finally, we add an ``effective compute gap" $\overline{E}_{\text{gap}}$ to our model to represent the possibility that the automated coder milestone may come later than when $H_{AC}$ is achieved (e.g. maybe good performance on long-horizon benchmark coding tasks don't by themselves translate to reliable performance on high-context real-world coding tasks). That is, we just add the parameter $\overline{E}_{\text{gap}}$ to our initial estimate for $\overline{E}_{AC}$ that only relied on time horizon.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{time_horizon_growth.png}
  %  \caption{Illustrative graph showing illustrating how setting the Automated coder effective compute requirement works in our model.}
\end{figure}

\textbf{Expandable: Explicit formula for $\overline{E}_{AC}$ with an effective compute gap.}
\[     \overline{E}_{AC}=
    \begin{cases}
        \overline{E}(t_p) + \frac{\tau}{d-1}\left(\left(\frac{H_{AC}}{H(t_p)}\right)^{\log_2 d}-1\right) + \overline{E}_{\text{gap}} & \text{ if } d \neq 1; \\
        \overline{E}(t_p) + \tau \,\log_2\!\left(\frac{H_{AC}}{H(t_p)}\right) + \overline{E}_{\text{gap}} & \text{ if } d=1.
    \end{cases}\]
\textbf{End of expandable}

We estimate the chance of a gap, size of the gap, and the pre-gap time horizon by reasoning about why there might be a gap and making intuitive estimates informed by this reasoning.

\subsection{Coding automation fraction and efficiency} \label{Coding automation fraction and efficiency}

\subsubsection{Coding automation fraction} \label{Coding automation fraction}

The coding automation fraction, $A(t)$, is the fraction of coding work involved in frontier AI research that can be efficiently automated. 

\textbf{Expandable: Clone-parity coders.} We define a ``clone-parity coder" by the condition that, if every human coder were replaced by a clone-parity coder, productivity would remain the same. For each type of coding task, we weight its importance by how many clone-parity coders would work on the task absent AI assistance. We consider a task efficiently automated when AIs can complete the task faster than the human coders and using no more than $\frac{B}{N}$ of present-day automation compute, where $B$ is the number of human clone-parity-coder-equivalents working on the task today and $N$ is the total number of human coders today.

For example, $A(\text{March 2029}) = 80\%$ means that we could use the AIs of March 2029 to automate away $80\%$ of the coding work done at the baseline-date and not lose in productivity ($80\%$ as measured by the proportion of clone-parity-coders doing that work).
\textbf{End of expandable}

%\ANOTE{There's a parameter that I missed: $A(t_p)$. Say more about how this value is chosen.}

Our definition of the Automated Coder milestone does not technically imply that $A(t) = 1$ when the milestone is reached. Nevertheless, we will make the simplifying assumption that this is the case.

{\color{orange}Eli suggestion:} I think you should look at my GDoc section on the expandable below. I disagree that it has to be a small proportion. See \href{https://docs.google.com/document/d/1wsS2U4IG6k3C3wOzbNvzsljRuoaX_MqRG2NNZQEmER4/edit?tab=t.0#heading=h.mqw7aoro2njm}{here}.

\textbf{Expandable: Why doesn't the definition of AC literally imply all tasks are efficiently automated?} The Automated Coder (AC) milestone was defined by the condition that if it was dropped into the present day, it would overall be as productive as all present human coders with no AI assistance. This can be true even if it's still less efficient than humans for \textit{some} small proportion of tasks. \textbf{End of expandable}

We model coding automation fraction $A(t)$ as a linear function of OOMs of effective compute $\overline{E}(t) = \log E(t)$. That is, we estimate the automation fraction $A(t_p)$ in the present, and let $A(t)$ vary linearly as a function of $\overline{E}(t)$ until $A(t)$ reaches 1 when $\overline{E}(t)$ equals the effective compute $\overline{E}_{AC}$ for the automated coder anchor. So:
\[ \text{$\overline{E}(t)$ is $y \%$ of the way between $\overline{E}(t_p)$ and $\overline{E}_{AC}$} \quad \xrightarrow{\text{means}} \quad \text{$A(t)$ is $y \%$ of the way between $A(t_p)$ and $1$}\]

\subsubsection{Coding automation efficiency} \label{Coding automation efficiency}

We use indices $i$ in the interval $[0,1]$ to represent the various coding tasks involved in implementing research experiments. At any given time $t$, tasks $i$ such that $0 \leq i \leq A(t)$ are considered efficiently automated, while tasks $i$ such that $A(t) < i \leq 1$ are not.

For each task $i$, the coding automation efficiency, $\eta_i(t)$ is the conversion rate between units of automation compute and human coders. So, if $C_{\text{aut}, i}(t)$ is the amount of compute assigned to task $i$ at time $t$, then this corresponds to $\eta_i(t) \, C_{\text{aut}, i}(t)$ human coders. $\eta_i(t)$ is basically a measure of the runtime efficiency of AIs for task $i$.

Writing $E_i$ for the effective compute corresponding to when task $i$ is first efficiently automated, we use
\[ \eta_i(t) = \begin{cases} \eta_{\text{init}} \cdot \left(\frac{E(t)}{E_i}\right)^{\eta_{\text{slope}}} & \text{if task $i$ is efficiently automated by time $t$;}\\ 0 & \text{otherwise;} \end{cases}\]
where $\eta_{\text{init}}$ is a parameter representing the initial efficiency when any task first gets automated, and $\eta_{\text{slope}}$ is a parameter controlling how quickly automated coders’ efficiency continues to improve afterward. This can be intuitively understood as:
\[ \text{each doubling of effective compute $E$} \quad \xrightarrow{\text{produces}} \quad \text{$\eta_{\text{slope}}$ doublings of coding automation efficiency}\]
These efficiency gains come from a combination of making AIs faster, being able to run more AIs, and getting more capable AIs that can complete tasks with fewer actions. By our definition of when a coding task is efficiently automated, the initial conversion rate $\eta_{\text{init}}$ is the ratio of the total number of human coders to the amount of automation compute in the baseline year.

\subsection{Automated experiment selection skill} \label{Automated experiment selection skill}

\BTODO{Update this section given changes made re: algorithmic limits}

Experiment selection skill measures the value one gets per experiment. We modeled \hyperref[Human experiment selection skill]{human experiment selection skill} with a lognormal distribution $\text{Lognormal}\left(\mu, \sigma^2 \right)$.

We describe the AIs' experiment selection skill in terms of standard deviations $\sigma$ of the (underlying normal of the) human distribution. Write $T_{AI}^{SD}(t)$ for the number of human standard deviations from the human median (if $T_{AI}^{SD}(t)$ is negative, this means the AIs' experiment selection skill is below the human median). We assume the AIs' experiment selection skill increases by a fixed rate $T_{\text{rate}}$ for each additional OOM of effective compute:
\[ T_{AI}^{SD}(t) = T_{AC} + T_{\text{rate}} \left( \overline{E}(t) - \overline{E}_{AC} \right) \quad \text{in human standard deviations $\sigma$}.\]
Here $T_{AC}$ is a model parameter for the experiment selection skill of the AIs at \hyperref[Automated coder]{Automated Coder milestone}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{T_AI.png}
    \caption{$T_{AI}^{SD} = 2.5$ means the AIs' skill is 2.5 standard deviations above the human median \color{red}{(Modified a picture of a normal distribution from the internet)}}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Stage2.png}
%    \caption{Three possibilites for once AIs have far surpassed humans at long-horizon agency.}
\end{figure}

Moving from standard deviations to absolute units, the AIs' experiment selection skill is
\[ T_{AI}(t) = \exp\left(\mu + \sigma \, T_{AI}^{SD}(t) \right).\] 



























\end{document}